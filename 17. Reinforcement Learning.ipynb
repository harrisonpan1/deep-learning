{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d3d78a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import random\n",
    "import numpy as np\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185ce0f7",
   "metadata": {},
   "source": [
    "# 17.1 Markov Decision Process (MDP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507203a1",
   "metadata": {},
   "source": [
    "Markov decision process (MDP), \n",
    "$$\\textrm{MDP}: (\\mathcal{S}, \\mathcal{A}, T, r)$$\n",
    "-  $S$ be the set of states in the MDP\n",
    "- $A$ be the set of actions that the robot can take at each state. Actions can change the current state of the robot to some other state within the set $S$\n",
    "- Transition Function, $T: \\mathcal{S} \\times \\mathcal{A} \\times \\mathcal{S} \\to [0,1]$ such that $T(s, a, s') = P(s' \\mid s, a)$ using the conditional probability of reaching a state $s'$ given that the robot was at state $s$ and took an action $a$\n",
    "    - $\\sum_{s' \\in \\mathcal{S}} T(s, a, s') = 1$ for all $s \\in \\mathcal{S}$ and $a \\in \\mathcal{A}$, i.e., the robot has to go to some state if it takes an action\n",
    "-  Reward, $r: \\mathcal{S} \\times \\mathcal{A} \\to \\mathbb{R}$. We say that the robot gets a reward $r(s,a)$ if the robot takes an action $a$ at state $s$ \n",
    "- Trajectory, $\\tau = (s_0, a_0, r_0, s_1, a_1, r_1, s_2, a_2, r_2, \\ldots)$\n",
    "    - At each time step $t$ the robot is at a state $s_t$ and takes an action $a_t$ which results in a reward $r_t = r(s_t, a_t)$. The discounted return of a trajectory is the total reward obtained by the robot along such a trajectory\n",
    "    $$R(\\tau) = r_0 + \\gamma r_1 + \\gamma^2 r_2 + \\cdots = \\sum_{t=0}^\\infty \\gamma^t r_t$$\n",
    "    - goal of RL is to find a trajectory that has the largest return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83faef4",
   "metadata": {},
   "source": [
    "# 17.2 Value Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a513ee64",
   "metadata": {},
   "source": [
    "- Stochastic Policy denoted as $\\pi(a \\mid s)$ (policy for short) is a conditional distribution over the actions $a \\in \\mathcal{A}$ given the state $s \\in \\mathcal{S}$, $\\pi(a \\mid s) \\equiv P(a \\mid s)$\n",
    "    - $\\sum_a \\pi(a \\mid s) = 1$ for any state $s$\n",
    "    - optimal policy, $\\pi^* = \\underset{\\pi}{\\mathrm{argmax}} V^\\pi(s_0).$\n",
    "- Value Function - the expected $\\gamma$-discounted return obtained by the robot if it begins at state $s_0$ and takes actions from the policy $\\pi$ at each time instant.\n",
    "$$V^\\pi(s_0) = E_{a_t \\sim \\pi(s_t)} \\Big[ R(\\tau) \\Big] = E_{a_t \\sim \\pi(s_t)} \\Big[ \\sum_{t=0}^\\infty \\gamma^t r(s_t, a_t) \\Big] = r(s_0, a_0) + \\gamma\\ E_{a_0 \\sim \\pi(s_0)} \\Big[ E_{s_1 \\sim P(s_1 \\mid s_0, a_0)} \\Big[ V^\\pi(s_1) \\Big] \\Big].$$\n",
    "$$V^\\pi(s) = \\sum_{a \\in \\mathcal{A}} \\pi(a \\mid s) \\Big[ r(s,  a) + \\gamma\\  \\sum_{s' \\in \\mathcal{S}} P(s' \\mid s, a) V^\\pi(s') \\Big];\\ \\textrm{for all } s \\in \\mathcal{S}.$$\n",
    "- Action-Value Function - the average return of a trajectory that begins at $s_0$ but when the action of the first stage is fixed to be $a_0$\n",
    "$$Q^\\pi(s_0, a_0) = r(s_0, a_0) + E_{a_t \\sim \\pi(s_t)} \\Big[ \\sum_{t=1}^\\infty \\gamma^t r(s_t, a_t) \\Big],$$\n",
    "$$Q^\\pi(s, a) = r(s, a) + \\gamma \\sum_{s' \\in \\mathcal{S}} P(s' \\mid s, a) \\sum_{a' \\in \\mathcal{A}} \\pi(a' \\mid s')\\ Q^\\pi(s', a');\\ \\textrm{ for all } s \\in \\mathcal{S}, a \\in \\mathcal{A}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56fa9450",
   "metadata": {},
   "source": [
    "## 17.6&7 Value Iteration & Policy Evalutation\n",
    "- value iteration$$V_{k+1}(s) = \\max_{a \\in \\mathcal{A}} \\Big\\{ r(s,  a) + \\gamma\\  \\sum_{s' \\in \\mathcal{S}} P(s' \\mid s, a) V_k(s') \\Big\\};\\ \\textrm{for all } s \\in \\mathcal{S}.$$\n",
    "    - as $k \\to \\infty$ the value function estimated by the Value Iteration algorithm converges to the optimal value function irrespective of the initialization $V_0$\n",
    "    $$V^*(s) = \\lim_{k \\to \\infty} V_k(s);\\ \\textrm{for all states } s \\in \\mathcal{S}.$$\n",
    "- action-value function iteration \n",
    "$$Q_{k+1}(s, a) = r(s, a) + \\gamma \\max_{a' \\in \\mathcal{A}} \\sum_{s' \\in \\mathcal{S}} P(s' \\mid s, a) Q_k (s', a');\\ \\textrm{ for all } s \\in \\mathcal{S}, a \\in \\mathcal{A}.$$\n",
    "$$Q^*(s, a) = \\lim_{k \\to \\infty} Q_k(s, a)$$\n",
    "- Policy Evaluation\n",
    "$$V^\\pi_{k+1}(s) = \\sum_{a \\in \\mathcal{A}} \\pi(a \\mid s) \\Big[ r(s,  a) + \\gamma\\  \\sum_{s' \\in \\mathcal{S}} P(s' \\mid s, a) V^\\pi_k(s') \\Big];\\ \\textrm{for all } s \\in \\mathcal{S}.$$\n",
    "$$V^\\pi(s) = \\lim_{k \\to \\infty} V^\\pi_k(s);\\ \\textrm{for all states } s \\in \\mathcal{S}.$$\n",
    "- !!!! implementing the Value Iteration algorithm requires that we know the Markov decision process (MDP), e.g., the transition and reward functions, completely.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83507957",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0  # Random number generator seed\n",
    "gamma = 0.95  # Discount factor\n",
    "num_iters = 10  # Number of iterations\n",
    "random.seed(seed)  # Set the random seed to ensure results can be reproduced\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Now set up the environment\n",
    "env_info = d2l.make_env(\"FrozenLake-v1\", seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f82877",
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(env_info, gamma, num_iters):\n",
    "    env_desc = env_info[\"desc\"]  # 2D array shows what each item means\n",
    "    prob_idx = env_info[\"trans_prob_idx\"]\n",
    "    nextstate_idx = env_info[\"nextstate_idx\"]\n",
    "    reward_idx = env_info[\"reward_idx\"]\n",
    "    num_states = env_info[\"num_states\"]\n",
    "    num_actions = env_info[\"num_actions\"]\n",
    "    mdp = env_info[\"mdp\"]\n",
    "\n",
    "    V = np.zeros((num_iters + 1, num_states))\n",
    "    Q = np.zeros((num_iters + 1, num_states, num_actions))\n",
    "    pi = np.zeros((num_iters + 1, num_states))\n",
    "\n",
    "    for k in range(1, num_iters + 1):\n",
    "        for s in range(num_states):\n",
    "            for a in range(num_actions):\n",
    "                # Calculate \\sum_{s'} p(s'\\mid s,a) [r + \\gamma v_k(s')]\n",
    "                for pxrds in mdp[(s, a)]:\n",
    "                    # mdp(s,a): [(p1,next1,r1,d1),(p2,next2,r2,d2),..]\n",
    "                    pr = pxrds[prob_idx]  # p(s'\\mid s,a)\n",
    "                    nextstate = pxrds[nextstate_idx]  # Next state\n",
    "                    reward = pxrds[reward_idx]  # Reward\n",
    "                    Q[k, s, a] += pr * (reward + gamma * V[k - 1, nextstate])\n",
    "            # Record max value and max action\n",
    "            V[k, s] = np.max(Q[k, s, :])\n",
    "            pi[k, s] = np.argmax(Q[k, s, :])\n",
    "    d2l.show_value_function_progress(env_desc, V[:-1], pi[:-1])\n",
    "\n",
    "\n",
    "value_iteration(env_info=env_info, gamma=gamma, num_iters=num_iters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00559be3",
   "metadata": {},
   "source": [
    "# 17.3 Q-Learning (类似MC simulation和greedy algo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ecb5fba",
   "metadata": {},
   "source": [
    "- Q-Learning is an algorithm to learn the value function without necessarily knowing the MDP\n",
    "- approximate version of Q function\n",
    "$$\\hat{Q} = \\min_Q \\underbrace{\\frac{1}{nT} \\sum_{i=1}^n \\sum_{t=0}^{T-1} (Q(s_t^i, a_t^i) - r(s_t^i, a_t^i) - \\gamma \\max_{a'} Q(s_{t+1}^i, a'))^2}_{\\stackrel{\\textrm{def}}{=} \\ell(Q)}.$$\n",
    "- update Q function $$\\begin{split}\\begin{aligned}Q(s_t^i, a_t^i) &\\leftarrow Q(s_t^i, a_t^i) - \\alpha \\nabla_{Q(s_t^i,a_t^i)} \\ell(Q) \n",
    "\\\\&=(1 - \\alpha) Q(s_t^i,a_t^i) - \\alpha \\Big( r(s_t^i, a_t^i) + \\gamma (1 - \\mathbb{1}_{s_{t+1}^i \\textrm{ is terminal}} )\\max_{a'} Q(s_{t+1}^i, a') \\Big).,\\end{aligned}\\end{split}$$\n",
    "    -  The value of state-action tuples $(s, a)$ that are not a part of the dataset is set to $-\\infty$\n",
    "-  $\\epsilon$-greedy exploration policy\n",
    "$$\\begin{split}\\pi_e(a \\mid s) = \\begin{cases}\\mathrm{argmax}_{a'} \\hat{Q}(s, a') & \\textrm{with prob. } 1-\\epsilon \\\\ \\textrm{uniform}(\\mathcal{A}) & \\textrm{with prob. } \\epsilon,\\end{cases}\\end{split}$$\n",
    "- softmax exploration policy\n",
    "$$\\pi_e(a \\mid s) = \\frac{e^{\\hat{Q}(s, a)/T}}{\\sum_{a'} e^{\\hat{Q}(s, a')/T}};$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0853d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0  # Random number generator seed\n",
    "gamma = 0.95  # Discount factor\n",
    "num_iters = 256  # Number of iterations\n",
    "alpha = 0.9  # Learing rate\n",
    "epsilon = 0.9  # Epsilon in epsilion gready algorithm\n",
    "random.seed(seed)  # Set the random seed\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Now set up the environment\n",
    "env_info = d2l.make_env(\"FrozenLake-v1\", seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5617c4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def e_greedy(env, Q, s, epsilon):\n",
    "    if random.random() < epsilon:\n",
    "        return env.action_space.sample()\n",
    "\n",
    "    else:\n",
    "        return np.argmax(Q[s, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec20491",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning(env_info, gamma, num_iters, alpha, epsilon):\n",
    "    env_desc = env_info[\"desc\"]  # 2D array specifying what each grid item means\n",
    "    env = env_info[\"env\"]  # 2D array specifying what each grid item means\n",
    "    num_states = env_info[\"num_states\"]\n",
    "    num_actions = env_info[\"num_actions\"]\n",
    "\n",
    "    Q = np.zeros((num_states, num_actions))\n",
    "    V = np.zeros((num_iters + 1, num_states))\n",
    "    pi = np.zeros((num_iters + 1, num_states))\n",
    "\n",
    "    for k in range(1, num_iters + 1):\n",
    "        # Reset environment\n",
    "        state, done = env.reset(), False\n",
    "        while not done:\n",
    "            # Select an action for a given state and acts in env based on selected action\n",
    "            action = e_greedy(env, Q, state, epsilon)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "            # Q-update:\n",
    "            y = reward + gamma * np.max(Q[next_state, :])\n",
    "            Q[state, action] = Q[state, action] + alpha * (y - Q[state, action])\n",
    "\n",
    "            # Move to the next state\n",
    "            state = next_state\n",
    "        # Record max value and max action for visualization purpose only\n",
    "        for s in range(num_states):\n",
    "            V[k, s] = np.max(Q[s, :])\n",
    "            pi[k, s] = np.argmax(Q[s, :])\n",
    "    d2l.show_Q_function_progress(env_desc, V[:-1], pi[:-1])\n",
    "\n",
    "\n",
    "q_learning(\n",
    "    env_info=env_info, gamma=gamma, num_iters=num_iters, alpha=alpha, epsilon=epsilon\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
