{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "x4lslYRPjGRC"
      },
      "outputs": [],
      "source": [
        "# Basic libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import json\n",
        "import pickle\n",
        "\n",
        "# PyTorch related\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Transformers library\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from transformers import BertTokenizer, BertModel, BertForSequenceClassification\n",
        "\n",
        "# Date and time processing\n",
        "from pandas.tseries.offsets import DateOffset\n",
        "from dateutil.relativedelta import relativedelta\n",
        "\n",
        "# Progress bar\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Evaluation metrics\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "\n",
        "# Date offset\n",
        "from pandas.tseries.offsets import DateOffset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fa_rJWF5jGRF"
      },
      "source": [
        "# import data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "bF08YoYMjGRG"
      },
      "outputs": [],
      "source": [
        "news_df = pd.read_csv(r\"C:\\Users\\Admin\\Documents\\FA690_MiniProject2\\news.csv\")\n",
        "news_df[\"publication_datetime\"] = pd.to_datetime(news_df[\"publication_datetime\"])\n",
        "\n",
        "price_df = pd.read_csv(r\"C:\\Users\\Admin\\Documents\\FA690_MiniProject2\\price.csv\")\n",
        "price_df[\"daily_return\"] = price_df.groupby(\"ticker\")[\"close\"].transform(\n",
        "    lambda x: x.pct_change()\n",
        ")\n",
        "price_df = price_df.dropna()\n",
        "price_df[\"Date\"] = pd.to_datetime(price_df[\"Date\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "MRSqlhkcjGRG",
        "outputId": "940d1b9d-bfd4-4b48-fe7a-0d0f1d10a78f"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>publication_datetime</th>\n",
              "      <th>title</th>\n",
              "      <th>body</th>\n",
              "      <th>tickers</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2017-01-03</td>\n",
              "      <td>World News: Police Question Netanyahu Over Gifts</td>\n",
              "      <td>\"We pay attention to publications in the media...</td>\n",
              "      <td>EL</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2017-01-03</td>\n",
              "      <td>Business News: Nestle Turns to New CEO for Hea...</td>\n",
              "      <td>Nestle, the world's largest packaged-food comp...</td>\n",
              "      <td>GIS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2017-01-03</td>\n",
              "      <td>Business News: Vermont Drug Law Faces Limits -...</td>\n",
              "      <td>The Vermont law, enacted in June, instructed s...</td>\n",
              "      <td>ABBV</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2017-01-03</td>\n",
              "      <td>Life &amp; Arts -- Travel: How Hotel Companies Lau...</td>\n",
              "      <td>Travelers are about to see a flurry of new hot...</td>\n",
              "      <td>HLT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2017-01-03</td>\n",
              "      <td>Businesses Ready to Ramp Up Investment --- Aft...</td>\n",
              "      <td>The Federal Reserve last month signaled intere...</td>\n",
              "      <td>HD</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20545</th>\n",
              "      <td>2020-12-30</td>\n",
              "      <td>Terms in Google Ad Deal Revealed</td>\n",
              "      <td>Ten Republican attorneys general, led by Texas...</td>\n",
              "      <td>GOOGL</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20546</th>\n",
              "      <td>2020-12-30</td>\n",
              "      <td>'Son of Sam' Law Invoked in Phony Heiress Case</td>\n",
              "      <td>Last year, the New York state attorney general...</td>\n",
              "      <td>NFLX</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20547</th>\n",
              "      <td>2020-12-30</td>\n",
              "      <td>Boeing MAX Returns to U.S. Sky</td>\n",
              "      <td>Daily round-trip flights between Miami and New...</td>\n",
              "      <td>BA</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20548</th>\n",
              "      <td>2020-12-30</td>\n",
              "      <td>To Curb Ma's Empire, China Weighs Taking a Big...</td>\n",
              "      <td>The regulators, led by the central bank, also ...</td>\n",
              "      <td>PYPL</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20549</th>\n",
              "      <td>2020-12-30</td>\n",
              "      <td>Slack Deal Shows Handcuffs on Tech Firms</td>\n",
              "      <td>Salesforce.com's $27.7 billion acquisition of ...</td>\n",
              "      <td>CRM</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>20550 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      publication_datetime                                              title  \\\n",
              "0               2017-01-03   World News: Police Question Netanyahu Over Gifts   \n",
              "1               2017-01-03  Business News: Nestle Turns to New CEO for Hea...   \n",
              "2               2017-01-03  Business News: Vermont Drug Law Faces Limits -...   \n",
              "3               2017-01-03  Life & Arts -- Travel: How Hotel Companies Lau...   \n",
              "4               2017-01-03  Businesses Ready to Ramp Up Investment --- Aft...   \n",
              "...                    ...                                                ...   \n",
              "20545           2020-12-30                   Terms in Google Ad Deal Revealed   \n",
              "20546           2020-12-30     'Son of Sam' Law Invoked in Phony Heiress Case   \n",
              "20547           2020-12-30                     Boeing MAX Returns to U.S. Sky   \n",
              "20548           2020-12-30  To Curb Ma's Empire, China Weighs Taking a Big...   \n",
              "20549           2020-12-30           Slack Deal Shows Handcuffs on Tech Firms   \n",
              "\n",
              "                                                    body tickers  \n",
              "0      \"We pay attention to publications in the media...      EL  \n",
              "1      Nestle, the world's largest packaged-food comp...     GIS  \n",
              "2      The Vermont law, enacted in June, instructed s...    ABBV  \n",
              "3      Travelers are about to see a flurry of new hot...     HLT  \n",
              "4      The Federal Reserve last month signaled intere...      HD  \n",
              "...                                                  ...     ...  \n",
              "20545  Ten Republican attorneys general, led by Texas...   GOOGL  \n",
              "20546  Last year, the New York state attorney general...    NFLX  \n",
              "20547  Daily round-trip flights between Miami and New...      BA  \n",
              "20548  The regulators, led by the central bank, also ...    PYPL  \n",
              "20549  Salesforce.com's $27.7 billion acquisition of ...     CRM  \n",
              "\n",
              "[20550 rows x 4 columns]"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "news_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fjuFdJ2mjGRH"
      },
      "source": [
        "# Data Sampling\n",
        "Consider two sceneraios:\n",
        " - time-series split:  \n",
        "        Train: up to 2019-12-31  \n",
        "        Validation: 2020-01-01 to 2020-06-30  \n",
        "        Test: from 2020-07-01 onward\n",
        "    - The issue with this is that validation set has a different distribution with training set due to COVID\n",
        "- rolling-window split:  \n",
        "        Train: 12month  \n",
        "        Validation: 4month  \n",
        "        Test: 2month  \n",
        "        Rolling step: 2month"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "4jFW7iQMjGRH"
      },
      "outputs": [],
      "source": [
        "# Define window sizes\n",
        "train_months = 12\n",
        "val_months = 4\n",
        "test_months = 2\n",
        "step_months = 2\n",
        "\n",
        "# Find the earliest and latest datetime\n",
        "start_date = pd.to_datetime(\"2017-01-01\")\n",
        "end_date = pd.to_datetime(\"2020-12-31\")\n",
        "\n",
        "# Initialize result list\n",
        "rolling_splits = []\n",
        "\n",
        "current_start = start_date\n",
        "\n",
        "while True:\n",
        "    train_start = current_start\n",
        "    train_end = train_start + relativedelta(months=train_months) - pd.Timedelta(days=1)\n",
        "\n",
        "    val_start = train_end + pd.Timedelta(days=1)\n",
        "    val_end = val_start + relativedelta(months=val_months) - pd.Timedelta(days=1)\n",
        "\n",
        "    test_start = val_end + pd.Timedelta(days=1)\n",
        "    test_end = test_start + relativedelta(months=test_months) - pd.Timedelta(days=1)\n",
        "\n",
        "    # Stop if test_end exceeds data range\n",
        "    if test_end > end_date:\n",
        "        break\n",
        "\n",
        "    train_set = news_df[\n",
        "        (news_df[\"publication_datetime\"] >= train_start)\n",
        "        & (news_df[\"publication_datetime\"] <= train_end)\n",
        "    ]\n",
        "    val_set = news_df[\n",
        "        (news_df[\"publication_datetime\"] >= val_start)\n",
        "        & (news_df[\"publication_datetime\"] <= val_end)\n",
        "    ]\n",
        "    test_set = news_df[\n",
        "        (news_df[\"publication_datetime\"] >= test_start)\n",
        "        & (news_df[\"publication_datetime\"] <= test_end)\n",
        "    ]\n",
        "\n",
        "    years = {\n",
        "        \"train\": pd.date_range(train_start, train_end, freq=\"MS\")\n",
        "        .strftime(\"%Y-%m\")\n",
        "        .tolist(),\n",
        "        \"val\": pd.date_range(val_start, val_end, freq=\"MS\").strftime(\"%Y-%m\").tolist(),\n",
        "        \"test\": pd.date_range(test_start, test_end, freq=\"MS\")\n",
        "        .strftime(\"%Y-%m\")\n",
        "        .tolist(),\n",
        "    }\n",
        "\n",
        "    rolling_splits.append(\n",
        "        {\n",
        "            \"train_set\": train_set,\n",
        "            \"val_set\": val_set,\n",
        "            \"test_set\": test_set,\n",
        "            \"years\": years,\n",
        "        }\n",
        "    )\n",
        "\n",
        "    # Move forward by rolling step\n",
        "    current_start += relativedelta(months=step_months)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "qcpgTKaCjGRI",
        "outputId": "82703419-55f2-4058-c7e4-28c00b7ee1d5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'train_set':       publication_datetime                                              title  \\\n",
              " 13057           2019-07-01  Streetwise: Fund's Ethical Impact Is Difficult...   \n",
              " 13058           2019-07-01  Markets Review & Outlook: Second Quarter --- U...   \n",
              " 13059           2019-07-01  Markets Review & Outlook: Second Quarter --- U...   \n",
              " 13060           2019-07-01  Markets Review & Outlook: Second Quarter --- S...   \n",
              " 13061           2019-07-01  Jony Ive's Long Drift From Apple --- The desig...   \n",
              " ...                    ...                                                ...   \n",
              " 18034           2020-06-30  Business News: Cloud Arm Zeros In on Space Org...   \n",
              " 18035           2020-06-30  Business News: Cloud Arm Zeros In on Space Org...   \n",
              " 18036           2020-06-30             Casinos Are Sued Over Virus Protection   \n",
              " 18037           2020-06-30  AMC Delays Reopening Theaters As Studios Push ...   \n",
              " 18038           2020-06-30                                          Overheard   \n",
              " \n",
              "                                                     body tickers  \n",
              " 13057  Last week, the fund's ethics committee allowed...      PM  \n",
              " 13058  The latest example: U.S. drugmaker AbbVie Inc....     RTX  \n",
              " 13059  The latest example: U.S. drugmaker AbbVie Inc....    ABBV  \n",
              " 13060  Stocks in 2019 have rebounded from a December ...     BAC  \n",
              " 13061  For nearly three hours on that afternoon in Ja...    AAPL  \n",
              " ...                                                  ...     ...  \n",
              " 18034  Capitol Hill is pouring billions of dollars in...     LMT  \n",
              " 18035  Capitol Hill is pouring billions of dollars in...    AMZN  \n",
              " 18036  Culinary Union Local 226 and Bartenders Union ...     MGM  \n",
              " 18037  AMC says it will open 450 U.S. locations on Ju...       T  \n",
              " 18038  The pharmacy has started rolling out Stryx, a ...     CVS  \n",
              " \n",
              " [4982 rows x 4 columns],\n",
              " 'val_set':       publication_datetime                                              title  \\\n",
              " 18039           2020-07-01                     Online Shopping Benefits FedEx   \n",
              " 18040           2020-07-01  U.S. News: Facebook Bans Much of Boogaloo Move...   \n",
              " 18041           2020-07-01  Business News: Netflix to Shift Some Money to ...   \n",
              " 18042           2020-07-01  Business News: Microsoft to Give 25 Million Wo...   \n",
              " 18043           2020-07-01         Facebook Again Tries To Answer Its Critics   \n",
              " ...                    ...                                                ...   \n",
              " 19755           2020-10-31  EXCHANGE --- Altria Sharply Cuts Valuation of ...   \n",
              " 19756           2020-10-31  EXCHANGE --- Apollo's $433 Billion Makeover Ma...   \n",
              " 19757           2020-10-31  EXCHANGE --- Apollo's $433 Billion Makeover Ma...   \n",
              " 19758           2020-10-31  EXCHANGE --- KKR's Portfolio Propels Earnings ...   \n",
              " 19759           2020-10-31  EXCHANGE --- Heard on the Street: Exxon Stays ...   \n",
              " \n",
              "                                                     body tickers  \n",
              " 18039  FedEx posted a 3% drop in total revenue to $17...     FDX  \n",
              " 18040  The large segment \"is actively promoting viole...    META  \n",
              " 18041  Netflix itself has previously committed to don...    NFLX  \n",
              " 18042  The effort is a response to two long-term tren...    MSFT  \n",
              " 18043  It was the latest effort by Facebook to show t...    META  \n",
              " ...                                                  ...     ...  \n",
              " 19755  Either estimate is a sharp decline from the $3...      MO  \n",
              " 19756  This approach has helped to make Apollo an inv...      BX  \n",
              " 19757  This approach has helped to make Apollo an inv...     APO  \n",
              " 19758  The fair value of KKR's private-equity portfol...     KKR  \n",
              " 19759  The company reported better-than-expected reve...     XOM  \n",
              " \n",
              " [1721 rows x 4 columns],\n",
              " 'test_set':       publication_datetime                                              title  \\\n",
              " 19760           2020-11-02     U.S. News: Political Groups Skirt Facebook Ban   \n",
              " 19761           2020-11-02  Tech Workers, Free to Roam, Put Down Roots in ...   \n",
              " 19762           2020-11-02  Business News: Amazon Struggles to Lure Luxury...   \n",
              " 19763           2020-11-02              Dish Offers To Build 5G With Pentagon   \n",
              " 19764           2020-11-02  Drone Firms Aim to Ship Vaccines --- Walmart, ...   \n",
              " ...                    ...                                                ...   \n",
              " 20545           2020-12-30                   Terms in Google Ad Deal Revealed   \n",
              " 20546           2020-12-30     'Son of Sam' Law Invoked in Phony Heiress Case   \n",
              " 20547           2020-12-30                     Boeing MAX Returns to U.S. Sky   \n",
              " 20548           2020-12-30  To Curb Ma's Empire, China Weighs Taking a Big...   \n",
              " 20549           2020-12-30           Slack Deal Shows Handcuffs on Tech Firms   \n",
              " \n",
              "                                                     body tickers  \n",
              " 19760  The issue shows Facebook's continued challenge...    META  \n",
              " 19761  \"You do see some California hostility here,\" s...    META  \n",
              " 19762  It isn't the first time Amazon has tried to ad...    AMZN  \n",
              " 19763  \"There is a precedent for how the DoD can take...       T  \n",
              " 19764  Meanwhile, some startups have discussed with g...     WMT  \n",
              " ...                                                  ...     ...  \n",
              " 20545  Ten Republican attorneys general, led by Texas...   GOOGL  \n",
              " 20546  Last year, the New York state attorney general...    NFLX  \n",
              " 20547  Daily round-trip flights between Miami and New...      BA  \n",
              " 20548  The regulators, led by the central bank, also ...    PYPL  \n",
              " 20549  Salesforce.com's $27.7 billion acquisition of ...     CRM  \n",
              " \n",
              " [790 rows x 4 columns],\n",
              " 'years': {'train': ['2019-07',\n",
              "   '2019-08',\n",
              "   '2019-09',\n",
              "   '2019-10',\n",
              "   '2019-11',\n",
              "   '2019-12',\n",
              "   '2020-01',\n",
              "   '2020-02',\n",
              "   '2020-03',\n",
              "   '2020-04',\n",
              "   '2020-05',\n",
              "   '2020-06'],\n",
              "  'val': ['2020-07', '2020-08', '2020-09', '2020-10'],\n",
              "  'test': ['2020-11', '2020-12']}}"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rolling_splits[-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "edVM4vjcjGRI"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of splits: 16\n",
            "\n",
            "Split 1:\n",
            "  Train: 2017-01 to 2017-12 (12 months, 5015 articles)\n",
            "  Val:   2018-01 to 2018-04 (4 months, 1839 articles)\n",
            "  Test:  2018-05 to 2018-06 (2 months, 902 articles)\n",
            "\n",
            "Split 2:\n",
            "  Train: 2017-03 to 2018-02 (12 months, 5103 articles)\n",
            "  Val:   2018-03 to 2018-06 (4 months, 1916 articles)\n",
            "  Test:  2018-07 to 2018-08 (2 months, 848 articles)\n",
            "\n",
            "Split 3:\n",
            "  Train: 2017-05 to 2018-04 (12 months, 5308 articles)\n",
            "  Val:   2018-05 to 2018-08 (4 months, 1750 articles)\n",
            "  Test:  2018-09 to 2018-10 (2 months, 884 articles)\n",
            "\n",
            "Split 4:\n",
            "  Train: 2017-07 to 2018-06 (12 months, 5365 articles)\n",
            "  Val:   2018-07 to 2018-10 (4 months, 1732 articles)\n",
            "  Test:  2018-11 to 2018-12 (2 months, 832 articles)\n",
            "\n",
            "Split 5:\n",
            "  Train: 2017-09 to 2018-08 (12 months, 5350 articles)\n",
            "  Val:   2018-09 to 2018-12 (4 months, 1716 articles)\n",
            "  Test:  2019-01 to 2019-02 (2 months, 862 articles)\n",
            "\n",
            "Split 6:\n",
            "  Train: 2017-11 to 2018-10 (12 months, 5334 articles)\n",
            "  Val:   2018-11 to 2019-02 (4 months, 1694 articles)\n",
            "  Test:  2019-03 to 2019-04 (2 months, 949 articles)\n",
            "\n",
            "Split 7:\n",
            "  Train: 2018-01 to 2018-12 (12 months, 5305 articles)\n",
            "  Val:   2019-01 to 2019-04 (4 months, 1811 articles)\n",
            "  Test:  2019-05 to 2019-06 (2 months, 926 articles)\n",
            "\n",
            "Split 8:\n",
            "  Train: 2018-03 to 2019-02 (12 months, 5342 articles)\n",
            "  Val:   2019-03 to 2019-06 (4 months, 1875 articles)\n",
            "  Test:  2019-07 to 2019-08 (2 months, 929 articles)\n",
            "\n",
            "Split 9:\n",
            "  Train: 2018-05 to 2019-04 (12 months, 5277 articles)\n",
            "  Val:   2019-05 to 2019-08 (4 months, 1855 articles)\n",
            "  Test:  2019-09 to 2019-10 (2 months, 923 articles)\n",
            "\n",
            "Split 10:\n",
            "  Train: 2018-07 to 2019-06 (12 months, 5301 articles)\n",
            "  Val:   2019-07 to 2019-10 (4 months, 1852 articles)\n",
            "  Test:  2019-11 to 2019-12 (2 months, 749 articles)\n",
            "\n",
            "Split 11:\n",
            "  Train: 2018-09 to 2019-08 (12 months, 5382 articles)\n",
            "  Val:   2019-09 to 2019-12 (4 months, 1672 articles)\n",
            "  Test:  2020-01 to 2020-02 (2 months, 848 articles)\n",
            "\n",
            "Split 12:\n",
            "  Train: 2018-11 to 2019-10 (12 months, 5421 articles)\n",
            "  Val:   2019-11 to 2020-02 (4 months, 1597 articles)\n",
            "  Test:  2020-03 to 2020-04 (2 months, 810 articles)\n",
            "\n",
            "Split 13:\n",
            "  Train: 2019-01 to 2019-12 (12 months, 5338 articles)\n",
            "  Val:   2020-01 to 2020-04 (4 months, 1658 articles)\n",
            "  Test:  2020-05 to 2020-06 (2 months, 723 articles)\n",
            "\n",
            "Split 14:\n",
            "  Train: 2019-03 to 2020-02 (12 months, 5324 articles)\n",
            "  Val:   2020-03 to 2020-06 (4 months, 1533 articles)\n",
            "  Test:  2020-07 to 2020-08 (2 months, 855 articles)\n",
            "\n",
            "Split 15:\n",
            "  Train: 2019-05 to 2020-04 (12 months, 5185 articles)\n",
            "  Val:   2020-05 to 2020-08 (4 months, 1578 articles)\n",
            "  Test:  2020-09 to 2020-10 (2 months, 866 articles)\n",
            "\n",
            "Split 16:\n",
            "  Train: 2019-07 to 2020-06 (12 months, 4982 articles)\n",
            "  Val:   2020-07 to 2020-10 (4 months, 1721 articles)\n",
            "  Test:  2020-11 to 2020-12 (2 months, 790 articles)\n"
          ]
        }
      ],
      "source": [
        "# Print the number of splits\n",
        "print(f\"Number of splits: {len(rolling_splits)}\")\n",
        "\n",
        "# Summarize each split\n",
        "for i, split in enumerate(rolling_splits): \n",
        "    train_months = len(split['years']['train'])\n",
        "    val_months = len(split['years']['val'])\n",
        "    test_months = len(split['years']['test'])\n",
        "    \n",
        "    train_rows = len(split['train_set'])\n",
        "    val_rows = len(split['val_set'])\n",
        "    test_rows = len(split['test_set'])\n",
        "    \n",
        "    print(f\"\\nSplit {i+1}:\")\n",
        "    print(f\"  Train: {split['years']['train'][0]} to {split['years']['train'][-1]} ({train_months} months, {train_rows} articles)\")\n",
        "    print(f\"  Val:   {split['years']['val'][0]} to {split['years']['val'][-1]} ({val_months} months, {val_rows} articles)\")\n",
        "    print(f\"  Test:  {split['years']['test'][0]} to {split['years']['test'][-1]} ({test_months} months, {test_rows} articles)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Pretrained Large Language Model: FinBERT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0393c87e0f254c9885b85f7eaa0e9a58",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/226k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\huggingface_hub\\file_download.py:144: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Admin\\.cache\\huggingface\\hub\\models--yiyanghkust--finbert-tone. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
            "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
            "  warnings.warn(message)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4fc26b6167d648dcb68b3b81a69373de",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/533 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "898984b305da4ea49e8134bdb6e0793b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/439M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1a62c90ead2743c780162c766e08af3c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/439M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "import torch\n",
        "\n",
        "# Load FinBERT model and tokenizer: https://huggingface.co/yiyanghkust/finbert-tone\n",
        "finbert_tokenizer = BertTokenizer.from_pretrained('yiyanghkust/finbert-tone')\n",
        "finbert = BertForSequenceClassification.from_pretrained('yiyanghkust/finbert-tone')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The maximum length of input tokens for FinBERT: 512\n"
          ]
        }
      ],
      "source": [
        "# print the maximum length of input tokens\n",
        "print(f\"The maximum length of input tokens for FinBERT: {finbert.config.max_position_embeddings}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "def predict_sentiment(text):\n",
        "    inputs = finbert_tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
        "    with torch.no_grad():\n",
        "        outputs = finbert(**inputs)\n",
        "    scores = outputs.logits.softmax(dim=1).numpy()[0]\n",
        "    return scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>publication_datetime</th>\n",
              "      <th>title</th>\n",
              "      <th>body</th>\n",
              "      <th>tickers</th>\n",
              "      <th>neutral</th>\n",
              "      <th>positive</th>\n",
              "      <th>negative</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>20545</th>\n",
              "      <td>2020-12-30</td>\n",
              "      <td>Terms in Google Ad Deal Revealed</td>\n",
              "      <td>Ten Republican attorneys general, led by Texas...</td>\n",
              "      <td>GOOGL</td>\n",
              "      <td>0.233786</td>\n",
              "      <td>0.031028</td>\n",
              "      <td>0.735186</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20546</th>\n",
              "      <td>2020-12-30</td>\n",
              "      <td>'Son of Sam' Law Invoked in Phony Heiress Case</td>\n",
              "      <td>Last year, the New York state attorney general...</td>\n",
              "      <td>NFLX</td>\n",
              "      <td>0.999567</td>\n",
              "      <td>0.000021</td>\n",
              "      <td>0.000413</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20547</th>\n",
              "      <td>2020-12-30</td>\n",
              "      <td>Boeing MAX Returns to U.S. Sky</td>\n",
              "      <td>Daily round-trip flights between Miami and New...</td>\n",
              "      <td>BA</td>\n",
              "      <td>0.999890</td>\n",
              "      <td>0.000064</td>\n",
              "      <td>0.000045</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20548</th>\n",
              "      <td>2020-12-30</td>\n",
              "      <td>To Curb Ma's Empire, China Weighs Taking a Big...</td>\n",
              "      <td>The regulators, led by the central bank, also ...</td>\n",
              "      <td>PYPL</td>\n",
              "      <td>0.996614</td>\n",
              "      <td>0.002923</td>\n",
              "      <td>0.000463</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20549</th>\n",
              "      <td>2020-12-30</td>\n",
              "      <td>Slack Deal Shows Handcuffs on Tech Firms</td>\n",
              "      <td>Salesforce.com's $27.7 billion acquisition of ...</td>\n",
              "      <td>CRM</td>\n",
              "      <td>0.973938</td>\n",
              "      <td>0.024925</td>\n",
              "      <td>0.001138</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      publication_datetime                                              title  \\\n",
              "20545           2020-12-30                   Terms in Google Ad Deal Revealed   \n",
              "20546           2020-12-30     'Son of Sam' Law Invoked in Phony Heiress Case   \n",
              "20547           2020-12-30                     Boeing MAX Returns to U.S. Sky   \n",
              "20548           2020-12-30  To Curb Ma's Empire, China Weighs Taking a Big...   \n",
              "20549           2020-12-30           Slack Deal Shows Handcuffs on Tech Firms   \n",
              "\n",
              "                                                    body tickers   neutral  \\\n",
              "20545  Ten Republican attorneys general, led by Texas...   GOOGL  0.233786   \n",
              "20546  Last year, the New York state attorney general...    NFLX  0.999567   \n",
              "20547  Daily round-trip flights between Miami and New...      BA  0.999890   \n",
              "20548  The regulators, led by the central bank, also ...    PYPL  0.996614   \n",
              "20549  Salesforce.com's $27.7 billion acquisition of ...     CRM  0.973938   \n",
              "\n",
              "       positive  negative  \n",
              "20545  0.031028  0.735186  \n",
              "20546  0.000021  0.000413  \n",
              "20547  0.000064  0.000045  \n",
              "20548  0.002923  0.000463  \n",
              "20549  0.024925  0.001138  "
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Convert the list of scores into a DataFrame with separate columns\n",
        "sentiment_df = pd.DataFrame(news_df['body'].apply(predict_sentiment).tolist(), columns=['neutral', 'positive', 'negative'])\n",
        "\n",
        "# Append the new columns to news_df\n",
        "news_df = pd.concat([news_df.reset_index(drop=True), sentiment_df], axis=1)\n",
        "\n",
        "# save FinBERT sentiment\n",
        "news_df.to_csv(\"news_w_sentiment_FinBERT.csv\", index = False)\n",
        "news_df.tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Fine-tune BERT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertTokenizer, BertModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "# create the feature daily_return based on close price\n",
        "price_df['daily_return'] = price_df.groupby('ticker')['close'].transform(lambda x: x.pct_change())\n",
        "price_df = price_df.dropna()\n",
        "\n",
        "# Convert dates to datetime\n",
        "news_df['publication_datetime'] = pd.to_datetime(news_df['publication_datetime'])\n",
        "price_df['Date'] = pd.to_datetime(price_df['Date'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "57122ca12e684358afecf26c94a26d9c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\huggingface_hub\\file_download.py:144: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Admin\\.cache\\huggingface\\hub\\models--bert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
            "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
            "  warnings.warn(message)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f441dea50c884c2eab4ef8eac54d5aed",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a91513aa5a1f4895a3dadd2efda7bd38",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c05f844b4fcd4347badce896efba54ef",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b7ed440b4c374825bf4e9473874ec54c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "BertModel(\n",
              "  (embeddings): BertEmbeddings(\n",
              "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "    (position_embeddings): Embedding(512, 768)\n",
              "    (token_type_embeddings): Embedding(2, 768)\n",
              "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (encoder): BertEncoder(\n",
              "    (layer): ModuleList(\n",
              "      (0-11): 12 x BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSdpaSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (pooler): BertPooler(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (activation): Tanh()\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Initialize BERT tokenizer and model: https://huggingface.co/google-bert/bert-base-uncased\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
        "bert_model.eval()  # Set to evaluation mode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a custom dataset\n",
        "class NewsDataset(Dataset):\n",
        "    def __init__(self, news_data, price_data, tokenizer, bert_model, max_length=512, device=None):\n",
        "        self.news_data = news_data\n",
        "        self.price_data = price_data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.bert_model = bert_model\n",
        "        self.max_length = max_length\n",
        "        self.device = device or torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        \n",
        "        # Create a mapping of (date, ticker) to daily return\n",
        "        self.return_map = price_data.set_index(['Date', 'ticker'])['daily_return'].to_dict()\n",
        "        \n",
        "        # Pre-compute BERT embeddings for all articles\n",
        "        self.embeddings = []\n",
        "        \n",
        "        print(\"Computing BERT embeddings for all articles...\")\n",
        "        with torch.no_grad():\n",
        "            for idx in range(len(news_data)):\n",
        "                if idx % 1000 == 0:\n",
        "                    print(f\"Processing article {idx}/{len(news_data)}\")\n",
        "                    \n",
        "                text = news_data.iloc[idx]['body']\n",
        "                inputs = self.tokenizer(text, \n",
        "                                      return_tensors='pt',\n",
        "                                      max_length=self.max_length,\n",
        "                                      padding='max_length',\n",
        "                                      truncation=True)\n",
        "                \n",
        "                # 将输入数据移到与模型相同的设备上\n",
        "                inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
        "                \n",
        "                outputs = self.bert_model(**inputs)\n",
        "                # Use [CLS] token embedding as the document embedding\n",
        "                embedding = outputs.last_hidden_state[:, 0, :].squeeze().cpu().numpy()\n",
        "                self.embeddings.append(embedding)\n",
        "                \n",
        "        self.embeddings = np.array(self.embeddings)\n",
        "        print(\"Finished computing embeddings.\")\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.news_data)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        news_row = self.news_data.iloc[idx]\n",
        "        date = news_row['publication_datetime']\n",
        "        ticker = news_row['tickers']\n",
        "        \n",
        "        # Get the next trading day's return\n",
        "        next_trading_days = self.price_data[\n",
        "            (self.price_data['Date'] >= date) & \n",
        "            (self.price_data['ticker'] == ticker)\n",
        "        ]['Date'].unique()\n",
        "        \n",
        "        if len(next_trading_days) > 0:\n",
        "            next_date = next_trading_days[0]\n",
        "            daily_return = self.return_map.get((next_date, ticker), 0)\n",
        "        else:\n",
        "            daily_return = 0\n",
        "            \n",
        "        # Get pre-computed BERT embedding\n",
        "        bert_embedding = self.embeddings[idx]\n",
        "        \n",
        "        return torch.FloatTensor(bert_embedding), torch.FloatTensor([daily_return])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the neural network\n",
        "class ReturnPredictor(nn.Module):\n",
        "    def __init__(self, input_size=768, hidden_size=256):  # BERT base has 768 dimensions\n",
        "        super(ReturnPredictor, self).__init__()\n",
        "        self.network = nn.Sequential(\n",
        "            nn.Linear(input_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(hidden_size, hidden_size // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(hidden_size // 2, 1)\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.network(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating datasets for rolling splits...\n",
            "Creating split 1/16...\n",
            "Computing BERT embeddings for all articles...\n",
            "Processing article 0/5015\n",
            "Processing article 1000/5015\n",
            "Processing article 2000/5015\n",
            "Processing article 3000/5015\n",
            "Processing article 4000/5015\n",
            "Processing article 5000/5015\n",
            "Finished computing embeddings.\n",
            "Computing BERT embeddings for all articles...\n",
            "Processing article 0/1839\n",
            "Processing article 1000/1839\n",
            "Finished computing embeddings.\n",
            "Computing BERT embeddings for all articles...\n",
            "Processing article 0/902\n",
            "Finished computing embeddings.\n",
            "Creating split 2/16...\n",
            "Computing BERT embeddings for all articles...\n",
            "Processing article 0/5103\n",
            "Processing article 1000/5103\n",
            "Processing article 2000/5103\n",
            "Processing article 3000/5103\n",
            "Processing article 4000/5103\n",
            "Processing article 5000/5103\n",
            "Finished computing embeddings.\n",
            "Computing BERT embeddings for all articles...\n",
            "Processing article 0/1916\n",
            "Processing article 1000/1916\n",
            "Finished computing embeddings.\n",
            "Computing BERT embeddings for all articles...\n",
            "Processing article 0/848\n",
            "Finished computing embeddings.\n",
            "Creating split 3/16...\n",
            "Computing BERT embeddings for all articles...\n",
            "Processing article 0/5308\n",
            "Processing article 1000/5308\n",
            "Processing article 2000/5308\n",
            "Processing article 3000/5308\n",
            "Processing article 4000/5308\n",
            "Processing article 5000/5308\n",
            "Finished computing embeddings.\n",
            "Computing BERT embeddings for all articles...\n",
            "Processing article 0/1750\n",
            "Processing article 1000/1750\n",
            "Finished computing embeddings.\n",
            "Computing BERT embeddings for all articles...\n",
            "Processing article 0/884\n",
            "Finished computing embeddings.\n",
            "Creating split 4/16...\n",
            "Computing BERT embeddings for all articles...\n",
            "Processing article 0/5365\n",
            "Processing article 1000/5365\n",
            "Processing article 2000/5365\n",
            "Processing article 3000/5365\n",
            "Processing article 4000/5365\n",
            "Processing article 5000/5365\n",
            "Finished computing embeddings.\n",
            "Computing BERT embeddings for all articles...\n",
            "Processing article 0/1732\n",
            "Processing article 1000/1732\n",
            "Finished computing embeddings.\n",
            "Computing BERT embeddings for all articles...\n",
            "Processing article 0/832\n",
            "Finished computing embeddings.\n",
            "Creating split 5/16...\n",
            "Computing BERT embeddings for all articles...\n",
            "Processing article 0/5350\n",
            "Processing article 1000/5350\n",
            "Processing article 2000/5350\n",
            "Processing article 3000/5350\n",
            "Processing article 4000/5350\n",
            "Processing article 5000/5350\n",
            "Finished computing embeddings.\n",
            "Computing BERT embeddings for all articles...\n",
            "Processing article 0/1716\n",
            "Processing article 1000/1716\n",
            "Finished computing embeddings.\n",
            "Computing BERT embeddings for all articles...\n",
            "Processing article 0/862\n",
            "Finished computing embeddings.\n",
            "Creating split 6/16...\n",
            "Computing BERT embeddings for all articles...\n",
            "Processing article 0/5334\n",
            "Processing article 1000/5334\n",
            "Processing article 2000/5334\n",
            "Processing article 3000/5334\n",
            "Processing article 4000/5334\n",
            "Processing article 5000/5334\n",
            "Finished computing embeddings.\n",
            "Computing BERT embeddings for all articles...\n",
            "Processing article 0/1694\n",
            "Processing article 1000/1694\n",
            "Finished computing embeddings.\n",
            "Computing BERT embeddings for all articles...\n",
            "Processing article 0/949\n",
            "Finished computing embeddings.\n",
            "Creating split 7/16...\n",
            "Computing BERT embeddings for all articles...\n",
            "Processing article 0/5305\n",
            "Processing article 1000/5305\n",
            "Processing article 2000/5305\n",
            "Processing article 3000/5305\n",
            "Processing article 4000/5305\n",
            "Processing article 5000/5305\n",
            "Finished computing embeddings.\n",
            "Computing BERT embeddings for all articles...\n",
            "Processing article 0/1811\n",
            "Processing article 1000/1811\n",
            "Finished computing embeddings.\n",
            "Computing BERT embeddings for all articles...\n",
            "Processing article 0/926\n",
            "Finished computing embeddings.\n",
            "Creating split 8/16...\n",
            "Computing BERT embeddings for all articles...\n",
            "Processing article 0/5342\n",
            "Processing article 1000/5342\n",
            "Processing article 2000/5342\n",
            "Processing article 3000/5342\n",
            "Processing article 4000/5342\n",
            "Processing article 5000/5342\n",
            "Finished computing embeddings.\n",
            "Computing BERT embeddings for all articles...\n",
            "Processing article 0/1875\n",
            "Processing article 1000/1875\n",
            "Finished computing embeddings.\n",
            "Computing BERT embeddings for all articles...\n",
            "Processing article 0/929\n",
            "Finished computing embeddings.\n",
            "Creating split 9/16...\n",
            "Computing BERT embeddings for all articles...\n",
            "Processing article 0/5277\n",
            "Processing article 1000/5277\n",
            "Processing article 2000/5277\n",
            "Processing article 3000/5277\n",
            "Processing article 4000/5277\n",
            "Processing article 5000/5277\n",
            "Finished computing embeddings.\n",
            "Computing BERT embeddings for all articles...\n",
            "Processing article 0/1855\n",
            "Processing article 1000/1855\n",
            "Finished computing embeddings.\n",
            "Computing BERT embeddings for all articles...\n",
            "Processing article 0/923\n",
            "Finished computing embeddings.\n",
            "Creating split 10/16...\n",
            "Computing BERT embeddings for all articles...\n",
            "Processing article 0/5301\n",
            "Processing article 1000/5301\n",
            "Processing article 2000/5301\n",
            "Processing article 3000/5301\n",
            "Processing article 4000/5301\n",
            "Processing article 5000/5301\n",
            "Finished computing embeddings.\n",
            "Computing BERT embeddings for all articles...\n",
            "Processing article 0/1852\n",
            "Processing article 1000/1852\n",
            "Finished computing embeddings.\n",
            "Computing BERT embeddings for all articles...\n",
            "Processing article 0/749\n",
            "Finished computing embeddings.\n",
            "Creating split 11/16...\n",
            "Computing BERT embeddings for all articles...\n",
            "Processing article 0/5382\n",
            "Processing article 1000/5382\n",
            "Processing article 2000/5382\n",
            "Processing article 3000/5382\n",
            "Processing article 4000/5382\n",
            "Processing article 5000/5382\n",
            "Finished computing embeddings.\n",
            "Computing BERT embeddings for all articles...\n",
            "Processing article 0/1672\n",
            "Processing article 1000/1672\n",
            "Finished computing embeddings.\n",
            "Computing BERT embeddings for all articles...\n",
            "Processing article 0/848\n",
            "Finished computing embeddings.\n",
            "Creating split 12/16...\n",
            "Computing BERT embeddings for all articles...\n",
            "Processing article 0/5421\n",
            "Processing article 1000/5421\n",
            "Processing article 2000/5421\n",
            "Processing article 3000/5421\n",
            "Processing article 4000/5421\n",
            "Processing article 5000/5421\n",
            "Finished computing embeddings.\n",
            "Computing BERT embeddings for all articles...\n",
            "Processing article 0/1597\n",
            "Processing article 1000/1597\n",
            "Finished computing embeddings.\n",
            "Computing BERT embeddings for all articles...\n",
            "Processing article 0/810\n",
            "Finished computing embeddings.\n",
            "Creating split 13/16...\n",
            "Computing BERT embeddings for all articles...\n",
            "Processing article 0/5338\n",
            "Processing article 1000/5338\n",
            "Processing article 2000/5338\n",
            "Processing article 3000/5338\n",
            "Processing article 4000/5338\n",
            "Processing article 5000/5338\n",
            "Finished computing embeddings.\n",
            "Computing BERT embeddings for all articles...\n",
            "Processing article 0/1658\n",
            "Processing article 1000/1658\n",
            "Finished computing embeddings.\n",
            "Computing BERT embeddings for all articles...\n",
            "Processing article 0/723\n",
            "Finished computing embeddings.\n",
            "Creating split 14/16...\n",
            "Computing BERT embeddings for all articles...\n",
            "Processing article 0/5324\n",
            "Processing article 1000/5324\n",
            "Processing article 2000/5324\n",
            "Processing article 3000/5324\n",
            "Processing article 4000/5324\n",
            "Processing article 5000/5324\n",
            "Finished computing embeddings.\n",
            "Computing BERT embeddings for all articles...\n",
            "Processing article 0/1533\n",
            "Processing article 1000/1533\n",
            "Finished computing embeddings.\n",
            "Computing BERT embeddings for all articles...\n",
            "Processing article 0/855\n",
            "Finished computing embeddings.\n",
            "Creating split 15/16...\n",
            "Computing BERT embeddings for all articles...\n",
            "Processing article 0/5185\n",
            "Processing article 1000/5185\n",
            "Processing article 2000/5185\n",
            "Processing article 3000/5185\n",
            "Processing article 4000/5185\n",
            "Processing article 5000/5185\n",
            "Finished computing embeddings.\n",
            "Computing BERT embeddings for all articles...\n",
            "Processing article 0/1578\n",
            "Processing article 1000/1578\n",
            "Finished computing embeddings.\n",
            "Computing BERT embeddings for all articles...\n",
            "Processing article 0/866\n",
            "Finished computing embeddings.\n",
            "Creating split 16/16...\n",
            "Computing BERT embeddings for all articles...\n",
            "Processing article 0/4982\n",
            "Processing article 1000/4982\n",
            "Processing article 2000/4982\n",
            "Processing article 3000/4982\n",
            "Processing article 4000/4982\n",
            "Finished computing embeddings.\n",
            "Computing BERT embeddings for all articles...\n",
            "Processing article 0/1721\n",
            "Processing article 1000/1721\n",
            "Finished computing embeddings.\n",
            "Computing BERT embeddings for all articles...\n",
            "Processing article 0/790\n",
            "Finished computing embeddings.\n",
            "Created 16 dataset splits\n"
          ]
        }
      ],
      "source": [
        "# Set random seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Move BERT model to GPU if available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "bert_model = bert_model.to(device)\n",
        "\n",
        "print(\"Creating datasets for rolling splits...\")\n",
        "# Create datasets based on the rolling splits defined earlier\n",
        "datasets = {}\n",
        "\n",
        "for i, split in enumerate(rolling_splits):\n",
        "    print(f\"Creating split {i+1}/{len(rolling_splits)}...\")\n",
        "    \n",
        "    # Create train dataset for this split\n",
        "    train_dataset_i = NewsDataset(\n",
        "        split['train_set'],\n",
        "        price_df,\n",
        "        tokenizer,\n",
        "        bert_model,\n",
        "        device=device  # Pass device parameter\n",
        "    )\n",
        "    \n",
        "    # Apply the same modification for validation and test datasets\n",
        "    val_dataset_i = NewsDataset(\n",
        "        split['val_set'],\n",
        "        price_df,\n",
        "        tokenizer,\n",
        "        bert_model,\n",
        "        device=device  # Pass device parameter\n",
        "    )\n",
        "    \n",
        "    test_dataset_i = NewsDataset(\n",
        "        split['test_set'],\n",
        "        price_df,\n",
        "        tokenizer,\n",
        "        bert_model,\n",
        "        device=device  # Pass device parameter\n",
        "    )\n",
        "    \n",
        "    # Store datasets for this split\n",
        "    datasets[f'split_{i+1}'] = {\n",
        "        'train': train_dataset_i,\n",
        "        'val': val_dataset_i,\n",
        "        'test': test_dataset_i\n",
        "    }\n",
        "\n",
        "print(f\"Created {len(datasets)} dataset splits\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Created data loaders for 16 splits\n"
          ]
        }
      ],
      "source": [
        "# Create data loaders for each split\n",
        "data_loaders = {}\n",
        "\n",
        "for split_name, split_datasets in datasets.items():\n",
        "    train_loader = DataLoader(split_datasets['train'], batch_size=32, shuffle=True)\n",
        "    val_loader = DataLoader(split_datasets['val'], batch_size=32)\n",
        "    test_loader = DataLoader(split_datasets['test'], batch_size=32)\n",
        "    \n",
        "    data_loaders[split_name] = {\n",
        "        'train': train_loader,\n",
        "        'val': val_loader,\n",
        "        'test': test_loader\n",
        "    }\n",
        "\n",
        "print(f\"Created data loaders for {len(data_loaders)} splits\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training on split_1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/10 [Train]: 100%|██████████| 157/157 [01:26<00:00,  1.82it/s, loss=0.0015]\n",
            "Epoch 1/10 [Val]: 100%|██████████| 58/58 [00:31<00:00,  1.83it/s, loss=0.0009]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/10], Train Loss: 0.0017, Val Loss: 0.0007, Time: 118.27s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/10 [Train]: 100%|██████████| 157/157 [01:27<00:00,  1.80it/s, loss=0.0006]\n",
            "Epoch 2/10 [Val]: 100%|██████████| 58/58 [00:31<00:00,  1.82it/s, loss=0.0010]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/10], Train Loss: 0.0009, Val Loss: 0.0007, Time: 118.97s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/10 [Train]: 100%|██████████| 157/157 [01:26<00:00,  1.81it/s, loss=0.0003]\n",
            "Epoch 3/10 [Val]: 100%|██████████| 58/58 [00:31<00:00,  1.82it/s, loss=0.0010]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [3/10], Train Loss: 0.0007, Val Loss: 0.0006, Time: 118.69s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/10 [Train]: 100%|██████████| 157/157 [01:29<00:00,  1.75it/s, loss=0.0017]\n",
            "Epoch 4/10 [Val]: 100%|██████████| 58/58 [00:32<00:00,  1.77it/s, loss=0.0011]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [4/10], Train Loss: 0.0006, Val Loss: 0.0007, Time: 122.19s\n",
            "Early stopping triggered after 4 epochs. No improvement for 3 epochs.\n",
            "Average epoch training time: 119.53s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Testing split_1: 100%|██████████| 29/29 [00:16<00:00,  1.79it/s, loss=0.0002]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "split_1 Test Loss: 0.0004, Epochs Trained: 4\n",
            "\n",
            "Training on split_2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/10 [Train]: 100%|██████████| 160/160 [01:30<00:00,  1.76it/s, loss=0.0017]\n",
            "Epoch 1/10 [Val]: 100%|██████████| 60/60 [00:33<00:00,  1.77it/s, loss=0.0006]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/10], Train Loss: 0.0014, Val Loss: 0.0006, Time: 124.96s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/10 [Train]: 100%|██████████| 160/160 [01:28<00:00,  1.81it/s, loss=0.0003]\n",
            "Epoch 2/10 [Val]: 100%|██████████| 60/60 [00:33<00:00,  1.79it/s, loss=0.0005]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/10], Train Loss: 0.0007, Val Loss: 0.0005, Time: 121.86s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/10 [Train]: 100%|██████████| 160/160 [01:27<00:00,  1.83it/s, loss=0.0011]\n",
            "Epoch 3/10 [Val]: 100%|██████████| 60/60 [00:32<00:00,  1.84it/s, loss=0.0005]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [3/10], Train Loss: 0.0006, Val Loss: 0.0005, Time: 119.94s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/10 [Train]: 100%|██████████| 160/160 [01:27<00:00,  1.83it/s, loss=0.0005]\n",
            "Epoch 4/10 [Val]: 100%|██████████| 60/60 [00:32<00:00,  1.84it/s, loss=0.0005]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [4/10], Train Loss: 0.0006, Val Loss: 0.0005, Time: 119.84s\n",
            "Early stopping triggered after 4 epochs. No improvement for 3 epochs.\n",
            "Average epoch training time: 121.65s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Testing split_2: 100%|██████████| 27/27 [00:14<00:00,  1.88it/s, loss=0.0002]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "split_2 Test Loss: 0.0008, Epochs Trained: 4\n",
            "\n",
            "Training on split_3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/10 [Train]: 100%|██████████| 166/166 [01:30<00:00,  1.83it/s, loss=0.0006]\n",
            "Epoch 1/10 [Val]: 100%|██████████| 55/55 [00:29<00:00,  1.85it/s, loss=0.0002]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/10], Train Loss: 0.0012, Val Loss: 0.0006, Time: 120.40s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/10 [Train]: 100%|██████████| 166/166 [01:30<00:00,  1.83it/s, loss=0.0006]\n",
            "Epoch 2/10 [Val]: 100%|██████████| 55/55 [00:29<00:00,  1.85it/s, loss=0.0001]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/10], Train Loss: 0.0007, Val Loss: 0.0006, Time: 120.47s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/10 [Train]: 100%|██████████| 166/166 [01:30<00:00,  1.83it/s, loss=0.0008]\n",
            "Epoch 3/10 [Val]: 100%|██████████| 55/55 [00:29<00:00,  1.85it/s, loss=0.0001]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [3/10], Train Loss: 0.0006, Val Loss: 0.0005, Time: 120.50s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/10 [Train]: 100%|██████████| 166/166 [01:30<00:00,  1.83it/s, loss=0.0005]\n",
            "Epoch 4/10 [Val]: 100%|██████████| 55/55 [00:29<00:00,  1.85it/s, loss=0.0001]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [4/10], Train Loss: 0.0005, Val Loss: 0.0005, Time: 120.30s\n",
            "Early stopping triggered after 4 epochs. No improvement for 3 epochs.\n",
            "Average epoch training time: 120.42s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Testing split_3: 100%|██████████| 28/28 [00:14<00:00,  1.87it/s, loss=0.0008]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "split_3 Test Loss: 0.0009, Epochs Trained: 4\n",
            "\n",
            "Training on split_4\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/10 [Train]: 100%|██████████| 168/168 [01:31<00:00,  1.84it/s, loss=0.0009]\n",
            "Epoch 1/10 [Val]: 100%|██████████| 55/55 [00:29<00:00,  1.87it/s, loss=0.0019]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/10], Train Loss: 0.0030, Val Loss: 0.0009, Time: 120.89s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/10 [Train]: 100%|██████████| 168/168 [01:31<00:00,  1.83it/s, loss=0.0015]\n",
            "Epoch 2/10 [Val]: 100%|██████████| 55/55 [00:29<00:00,  1.87it/s, loss=0.0016]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/10], Train Loss: 0.0013, Val Loss: 0.0009, Time: 120.96s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/10 [Train]: 100%|██████████| 168/168 [01:31<00:00,  1.83it/s, loss=0.0005]\n",
            "Epoch 3/10 [Val]: 100%|██████████| 55/55 [00:29<00:00,  1.86it/s, loss=0.0019]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [3/10], Train Loss: 0.0009, Val Loss: 0.0009, Time: 121.34s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/10 [Train]: 100%|██████████| 168/168 [01:31<00:00,  1.83it/s, loss=0.0004]\n",
            "Epoch 4/10 [Val]: 100%|██████████| 55/55 [00:29<00:00,  1.87it/s, loss=0.0021]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [4/10], Train Loss: 0.0008, Val Loss: 0.0009, Time: 121.10s\n",
            "Early stopping triggered after 4 epochs. No improvement for 3 epochs.\n",
            "Average epoch training time: 121.07s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Testing split_4: 100%|██████████| 26/26 [00:14<00:00,  1.84it/s, loss=0.0005]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "split_4 Test Loss: 0.0013, Epochs Trained: 4\n",
            "\n",
            "Training on split_5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/10 [Train]: 100%|██████████| 168/168 [01:31<00:00,  1.83it/s, loss=0.0011]\n",
            "Epoch 1/10 [Val]: 100%|██████████| 54/54 [00:29<00:00,  1.85it/s, loss=0.0004]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/10], Train Loss: 0.0019, Val Loss: 0.0012, Time: 120.97s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/10 [Train]: 100%|██████████| 168/168 [01:31<00:00,  1.83it/s, loss=0.0004]\n",
            "Epoch 2/10 [Val]: 100%|██████████| 54/54 [00:29<00:00,  1.86it/s, loss=0.0004]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/10], Train Loss: 0.0010, Val Loss: 0.0011, Time: 120.71s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/10 [Train]: 100%|██████████| 168/168 [01:31<00:00,  1.83it/s, loss=0.0010]\n",
            "Epoch 3/10 [Val]: 100%|██████████| 54/54 [00:29<00:00,  1.86it/s, loss=0.0005]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [3/10], Train Loss: 0.0008, Val Loss: 0.0011, Time: 120.65s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/10 [Train]: 100%|██████████| 168/168 [01:31<00:00,  1.84it/s, loss=0.0001]\n",
            "Epoch 4/10 [Val]: 100%|██████████| 54/54 [00:29<00:00,  1.86it/s, loss=0.0005]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [4/10], Train Loss: 0.0007, Val Loss: 0.0011, Time: 120.58s\n",
            "Early stopping triggered after 4 epochs. No improvement for 3 epochs.\n",
            "Average epoch training time: 120.73s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Testing split_5: 100%|██████████| 27/27 [00:14<00:00,  1.84it/s, loss=0.0012]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "split_5 Test Loss: 0.0018, Epochs Trained: 4\n",
            "\n",
            "Training on split_6\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/10 [Train]: 100%|██████████| 167/167 [01:31<00:00,  1.83it/s, loss=0.0010]\n",
            "Epoch 1/10 [Val]: 100%|██████████| 53/53 [00:28<00:00,  1.84it/s, loss=0.0013]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/10], Train Loss: 0.0023, Val Loss: 0.0016, Time: 120.18s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/10 [Train]: 100%|██████████| 167/167 [01:31<00:00,  1.83it/s, loss=0.0013]\n",
            "Epoch 2/10 [Val]: 100%|██████████| 53/53 [00:28<00:00,  1.84it/s, loss=0.0012]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/10], Train Loss: 0.0012, Val Loss: 0.0015, Time: 120.08s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/10 [Train]: 100%|██████████| 167/167 [01:31<00:00,  1.83it/s, loss=0.0004]\n",
            "Epoch 3/10 [Val]: 100%|██████████| 53/53 [00:28<00:00,  1.84it/s, loss=0.0012]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [3/10], Train Loss: 0.0009, Val Loss: 0.0016, Time: 119.91s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/10 [Train]: 100%|██████████| 167/167 [01:31<00:00,  1.83it/s, loss=0.0010]\n",
            "Epoch 4/10 [Val]: 100%|██████████| 53/53 [00:28<00:00,  1.84it/s, loss=0.0012]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [4/10], Train Loss: 0.0009, Val Loss: 0.0016, Time: 120.04s\n",
            "Early stopping triggered after 4 epochs. No improvement for 3 epochs.\n",
            "Average epoch training time: 120.05s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Testing split_6: 100%|██████████| 30/30 [00:16<00:00,  1.86it/s, loss=0.0009]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "split_6 Test Loss: 0.0007, Epochs Trained: 4\n",
            "\n",
            "Training on split_7\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/10 [Train]: 100%|██████████| 166/166 [01:30<00:00,  1.83it/s, loss=0.0031]\n",
            "Epoch 1/10 [Val]: 100%|██████████| 57/57 [00:30<00:00,  1.86it/s, loss=0.0009]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/10], Train Loss: 0.0017, Val Loss: 0.0012, Time: 121.39s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/10 [Train]: 100%|██████████| 166/166 [01:30<00:00,  1.83it/s, loss=0.0005]\n",
            "Epoch 2/10 [Val]: 100%|██████████| 57/57 [00:30<00:00,  1.85it/s, loss=0.0009]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/10], Train Loss: 0.0010, Val Loss: 0.0012, Time: 121.56s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/10 [Train]: 100%|██████████| 166/166 [01:30<00:00,  1.83it/s, loss=0.0003]\n",
            "Epoch 3/10 [Val]: 100%|██████████| 57/57 [00:30<00:00,  1.86it/s, loss=0.0009]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [3/10], Train Loss: 0.0009, Val Loss: 0.0011, Time: 121.54s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/10 [Train]: 100%|██████████| 166/166 [01:30<00:00,  1.83it/s, loss=0.0004]\n",
            "Epoch 4/10 [Val]: 100%|██████████| 57/57 [00:30<00:00,  1.86it/s, loss=0.0010]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [4/10], Train Loss: 0.0008, Val Loss: 0.0011, Time: 121.33s\n",
            "Early stopping triggered after 4 epochs. No improvement for 3 epochs.\n",
            "Average epoch training time: 121.46s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Testing split_7: 100%|██████████| 29/29 [00:15<00:00,  1.85it/s, loss=0.0003]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "split_7 Test Loss: 0.0005, Epochs Trained: 4\n",
            "\n",
            "Training on split_8\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/10 [Train]: 100%|██████████| 167/167 [01:31<00:00,  1.82it/s, loss=0.0017]\n",
            "Epoch 1/10 [Val]: 100%|██████████| 59/59 [00:31<00:00,  1.86it/s, loss=0.0004]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/10], Train Loss: 0.0024, Val Loss: 0.0006, Time: 123.40s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/10 [Train]: 100%|██████████| 167/167 [01:31<00:00,  1.83it/s, loss=0.0007]\n",
            "Epoch 2/10 [Val]: 100%|██████████| 59/59 [00:31<00:00,  1.86it/s, loss=0.0006]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/10], Train Loss: 0.0014, Val Loss: 0.0006, Time: 123.14s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/10 [Train]: 100%|██████████| 167/167 [01:31<00:00,  1.83it/s, loss=0.0005]\n",
            "Epoch 3/10 [Val]: 100%|██████████| 59/59 [00:31<00:00,  1.85it/s, loss=0.0006]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [3/10], Train Loss: 0.0012, Val Loss: 0.0006, Time: 123.18s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/10 [Train]: 100%|██████████| 167/167 [01:31<00:00,  1.83it/s, loss=0.0008]\n",
            "Epoch 4/10 [Val]: 100%|██████████| 59/59 [00:31<00:00,  1.86it/s, loss=0.0006]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [4/10], Train Loss: 0.0011, Val Loss: 0.0006, Time: 123.17s\n",
            "Early stopping triggered after 4 epochs. No improvement for 3 epochs.\n",
            "Average epoch training time: 123.22s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Testing split_8: 100%|██████████| 30/30 [00:15<00:00,  1.90it/s, loss=0.0000]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "split_8 Test Loss: 0.0006, Epochs Trained: 4\n",
            "\n",
            "Training on split_9\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/10 [Train]: 100%|██████████| 165/165 [01:30<00:00,  1.82it/s, loss=0.0011]\n",
            "Epoch 1/10 [Val]: 100%|██████████| 58/58 [00:31<00:00,  1.85it/s, loss=0.0006]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/10], Train Loss: 0.0027, Val Loss: 0.0008, Time: 121.83s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/10 [Train]: 100%|██████████| 165/165 [01:30<00:00,  1.82it/s, loss=0.0010]\n",
            "Epoch 2/10 [Val]: 100%|██████████| 58/58 [00:31<00:00,  1.84it/s, loss=0.0005]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/10], Train Loss: 0.0015, Val Loss: 0.0007, Time: 121.92s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/10 [Train]: 100%|██████████| 165/165 [01:30<00:00,  1.82it/s, loss=0.0008]\n",
            "Epoch 3/10 [Val]: 100%|██████████| 58/58 [00:31<00:00,  1.84it/s, loss=0.0006]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [3/10], Train Loss: 0.0013, Val Loss: 0.0007, Time: 122.05s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/10 [Train]: 100%|██████████| 165/165 [01:30<00:00,  1.83it/s, loss=0.0008]\n",
            "Epoch 4/10 [Val]: 100%|██████████| 58/58 [00:31<00:00,  1.83it/s, loss=0.0006]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [4/10], Train Loss: 0.0012, Val Loss: 0.0007, Time: 121.90s\n",
            "Early stopping triggered after 4 epochs. No improvement for 3 epochs.\n",
            "Average epoch training time: 121.92s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Testing split_9: 100%|██████████| 29/29 [00:15<00:00,  1.85it/s, loss=0.0005]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "split_9 Test Loss: 0.0015, Epochs Trained: 4\n",
            "\n",
            "Training on split_10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/10 [Train]: 100%|██████████| 166/166 [01:31<00:00,  1.81it/s, loss=0.0007]\n",
            "Epoch 1/10 [Val]: 100%|██████████| 58/58 [00:31<00:00,  1.83it/s, loss=0.0002]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/10], Train Loss: 0.0019, Val Loss: 0.0010, Time: 123.34s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/10 [Train]: 100%|██████████| 166/166 [01:31<00:00,  1.82it/s, loss=0.0014]\n",
            "Epoch 2/10 [Val]: 100%|██████████| 58/58 [00:31<00:00,  1.83it/s, loss=0.0003]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/10], Train Loss: 0.0013, Val Loss: 0.0010, Time: 123.06s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/10 [Train]: 100%|██████████| 166/166 [01:31<00:00,  1.81it/s, loss=0.0007]\n",
            "Epoch 3/10 [Val]: 100%|██████████| 58/58 [00:31<00:00,  1.83it/s, loss=0.0003]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [3/10], Train Loss: 0.0011, Val Loss: 0.0010, Time: 123.50s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/10 [Train]: 100%|██████████| 166/166 [01:31<00:00,  1.81it/s, loss=0.0016]\n",
            "Epoch 4/10 [Val]: 100%|██████████| 58/58 [00:31<00:00,  1.83it/s, loss=0.0003]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [4/10], Train Loss: 0.0011, Val Loss: 0.0010, Time: 123.68s\n",
            "Early stopping triggered after 4 epochs. No improvement for 3 epochs.\n",
            "Average epoch training time: 123.40s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Testing split_10: 100%|██████████| 24/24 [00:12<00:00,  1.87it/s, loss=0.0002]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "split_10 Test Loss: 0.0007, Epochs Trained: 4\n",
            "\n",
            "Training on split_11\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/10 [Train]: 100%|██████████| 169/169 [01:33<00:00,  1.81it/s, loss=0.0007]\n",
            "Epoch 1/10 [Val]: 100%|██████████| 53/53 [00:28<00:00,  1.85it/s, loss=0.0003]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/10], Train Loss: 0.0020, Val Loss: 0.0011, Time: 121.90s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/10 [Train]: 100%|██████████| 169/169 [01:33<00:00,  1.81it/s, loss=0.0012]\n",
            "Epoch 2/10 [Val]: 100%|██████████| 53/53 [00:28<00:00,  1.85it/s, loss=0.0001]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/10], Train Loss: 0.0012, Val Loss: 0.0010, Time: 122.25s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/10 [Train]: 100%|██████████| 169/169 [01:33<00:00,  1.81it/s, loss=0.0004]\n",
            "Epoch 3/10 [Val]: 100%|██████████| 53/53 [00:28<00:00,  1.85it/s, loss=0.0001]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [3/10], Train Loss: 0.0011, Val Loss: 0.0010, Time: 122.05s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/10 [Train]: 100%|██████████| 169/169 [01:33<00:00,  1.81it/s, loss=0.0005]\n",
            "Epoch 4/10 [Val]: 100%|██████████| 53/53 [00:28<00:00,  1.85it/s, loss=0.0001]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [4/10], Train Loss: 0.0010, Val Loss: 0.0010, Time: 122.10s\n",
            "Early stopping triggered after 4 epochs. No improvement for 3 epochs.\n",
            "Average epoch training time: 122.07s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Testing split_11: 100%|██████████| 27/27 [00:14<00:00,  1.86it/s, loss=0.0013]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "split_11 Test Loss: 0.0007, Epochs Trained: 4\n",
            "\n",
            "Training on split_12\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/10 [Train]: 100%|██████████| 170/170 [01:33<00:00,  1.81it/s, loss=0.0011]\n",
            "Epoch 1/10 [Val]: 100%|██████████| 50/50 [00:27<00:00,  1.83it/s, loss=0.0022]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/10], Train Loss: 0.0019, Val Loss: 0.0007, Time: 121.26s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/10 [Train]: 100%|██████████| 170/170 [01:33<00:00,  1.81it/s, loss=0.0013]\n",
            "Epoch 2/10 [Val]: 100%|██████████| 50/50 [00:27<00:00,  1.83it/s, loss=0.0019]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/10], Train Loss: 0.0013, Val Loss: 0.0007, Time: 121.27s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/10 [Train]: 100%|██████████| 170/170 [01:33<00:00,  1.81it/s, loss=0.0016]\n",
            "Epoch 3/10 [Val]: 100%|██████████| 50/50 [00:27<00:00,  1.83it/s, loss=0.0019]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [3/10], Train Loss: 0.0011, Val Loss: 0.0007, Time: 121.27s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/10 [Train]: 100%|██████████| 170/170 [01:34<00:00,  1.81it/s, loss=0.0004]\n",
            "Epoch 4/10 [Val]: 100%|██████████| 50/50 [00:27<00:00,  1.83it/s, loss=0.0019]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [4/10], Train Loss: 0.0011, Val Loss: 0.0007, Time: 121.57s\n",
            "Early stopping triggered after 4 epochs. No improvement for 3 epochs.\n",
            "Average epoch training time: 121.34s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Testing split_12: 100%|██████████| 26/26 [00:13<00:00,  1.87it/s, loss=0.0013]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "split_12 Test Loss: 0.0048, Epochs Trained: 4\n",
            "\n",
            "Training on split_13\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/10 [Train]: 100%|██████████| 167/167 [01:32<00:00,  1.80it/s, loss=0.0009]\n",
            "Epoch 1/10 [Val]: 100%|██████████| 52/52 [00:28<00:00,  1.83it/s, loss=0.0011]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/10], Train Loss: 0.0021, Val Loss: 0.0028, Time: 121.26s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/10 [Train]: 100%|██████████| 167/167 [01:32<00:00,  1.80it/s, loss=0.0008]\n",
            "Epoch 2/10 [Val]: 100%|██████████| 52/52 [00:28<00:00,  1.83it/s, loss=0.0013]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/10], Train Loss: 0.0013, Val Loss: 0.0028, Time: 121.27s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/10 [Train]: 100%|██████████| 167/167 [01:32<00:00,  1.81it/s, loss=0.0008]\n",
            "Epoch 3/10 [Val]: 100%|██████████| 52/52 [00:28<00:00,  1.83it/s, loss=0.0014]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [3/10], Train Loss: 0.0011, Val Loss: 0.0027, Time: 120.84s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/10 [Train]: 100%|██████████| 167/167 [01:32<00:00,  1.81it/s, loss=0.0008]\n",
            "Epoch 4/10 [Val]: 100%|██████████| 52/52 [00:28<00:00,  1.83it/s, loss=0.0015]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [4/10], Train Loss: 0.0010, Val Loss: 0.0027, Time: 120.81s\n",
            "Early stopping triggered after 4 epochs. No improvement for 3 epochs.\n",
            "Average epoch training time: 121.04s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Testing split_13: 100%|██████████| 23/23 [00:12<00:00,  1.85it/s, loss=0.0006]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "split_13 Test Loss: 0.0014, Epochs Trained: 4\n",
            "\n",
            "Training on split_14\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/10 [Train]: 100%|██████████| 167/167 [01:32<00:00,  1.81it/s, loss=0.0010]\n",
            "Epoch 1/10 [Val]: 100%|██████████| 48/48 [00:26<00:00,  1.82it/s, loss=0.0011]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/10], Train Loss: 0.0016, Val Loss: 0.0033, Time: 118.75s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/10 [Train]: 100%|██████████| 167/167 [01:32<00:00,  1.81it/s, loss=0.0004]\n",
            "Epoch 2/10 [Val]: 100%|██████████| 48/48 [00:26<00:00,  1.82it/s, loss=0.0013]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/10], Train Loss: 0.0010, Val Loss: 0.0033, Time: 118.48s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/10 [Train]: 100%|██████████| 167/167 [01:32<00:00,  1.81it/s, loss=0.0006]\n",
            "Epoch 3/10 [Val]: 100%|██████████| 48/48 [00:26<00:00,  1.82it/s, loss=0.0013]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [3/10], Train Loss: 0.0009, Val Loss: 0.0033, Time: 118.83s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/10 [Train]: 100%|██████████| 167/167 [01:32<00:00,  1.81it/s, loss=0.0010]\n",
            "Epoch 4/10 [Val]: 100%|██████████| 48/48 [00:26<00:00,  1.83it/s, loss=0.0013]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [4/10], Train Loss: 0.0008, Val Loss: 0.0032, Time: 118.64s\n",
            "Early stopping triggered after 4 epochs. No improvement for 3 epochs.\n",
            "Average epoch training time: 118.68s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Testing split_14: 100%|██████████| 27/27 [00:14<00:00,  1.83it/s, loss=0.0025]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "split_14 Test Loss: 0.0012, Epochs Trained: 4\n",
            "\n",
            "Training on split_15\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/10 [Train]: 100%|██████████| 163/163 [01:30<00:00,  1.81it/s, loss=0.0007]\n",
            "Epoch 1/10 [Val]: 100%|██████████| 50/50 [00:27<00:00,  1.84it/s, loss=0.0006]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/10], Train Loss: 0.0027, Val Loss: 0.0013, Time: 117.28s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/10 [Train]: 100%|██████████| 163/163 [01:30<00:00,  1.81it/s, loss=0.0374]\n",
            "Epoch 2/10 [Val]: 100%|██████████| 50/50 [00:27<00:00,  1.84it/s, loss=0.0005]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/10], Train Loss: 0.0021, Val Loss: 0.0014, Time: 117.14s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/10 [Train]: 100%|██████████| 163/163 [01:30<00:00,  1.81it/s, loss=0.0003]\n",
            "Epoch 3/10 [Val]: 100%|██████████| 50/50 [00:27<00:00,  1.84it/s, loss=0.0005]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [3/10], Train Loss: 0.0017, Val Loss: 0.0014, Time: 117.28s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/10 [Train]: 100%|██████████| 163/163 [01:29<00:00,  1.81it/s, loss=0.0001]\n",
            "Epoch 4/10 [Val]: 100%|██████████| 50/50 [00:27<00:00,  1.85it/s, loss=0.0005]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [4/10], Train Loss: 0.0016, Val Loss: 0.0014, Time: 117.09s\n",
            "Early stopping triggered after 4 epochs. No improvement for 3 epochs.\n",
            "Average epoch training time: 117.20s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Testing split_15: 100%|██████████| 28/28 [00:14<00:00,  1.88it/s, loss=0.0019]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "split_15 Test Loss: 0.0008, Epochs Trained: 4\n",
            "\n",
            "Training on split_16\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/10 [Train]: 100%|██████████| 156/156 [01:26<00:00,  1.81it/s, loss=0.0017]\n",
            "Epoch 1/10 [Val]: 100%|██████████| 54/54 [00:29<00:00,  1.83it/s, loss=0.0007]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/10], Train Loss: 0.0034, Val Loss: 0.0011, Time: 115.88s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/10 [Train]: 100%|██████████| 156/156 [01:26<00:00,  1.81it/s, loss=0.0010]\n",
            "Epoch 2/10 [Val]: 100%|██████████| 54/54 [00:29<00:00,  1.82it/s, loss=0.0009]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/10], Train Loss: 0.0022, Val Loss: 0.0011, Time: 116.04s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/10 [Train]: 100%|██████████| 156/156 [01:26<00:00,  1.80it/s, loss=0.0010]\n",
            "Epoch 3/10 [Val]: 100%|██████████| 54/54 [00:29<00:00,  1.83it/s, loss=0.0011]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [3/10], Train Loss: 0.0019, Val Loss: 0.0011, Time: 116.22s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/10 [Train]: 100%|██████████| 156/156 [01:26<00:00,  1.80it/s, loss=0.0050]\n",
            "Epoch 4/10 [Val]: 100%|██████████| 54/54 [00:29<00:00,  1.83it/s, loss=0.0012]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [4/10], Train Loss: 0.0018, Val Loss: 0.0011, Time: 116.04s\n",
            "Early stopping triggered after 4 epochs. No improvement for 3 epochs.\n",
            "Average epoch training time: 116.05s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Testing split_16: 100%|██████████| 25/25 [00:13<00:00,  1.84it/s, loss=0.0003]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "split_16 Test Loss: 0.0009, Epochs Trained: 4\n",
            "\n",
            "Training completed for all splits.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Train on each split\n",
        "results = {}\n",
        "\n",
        "for split_name, loaders in data_loaders.items():\n",
        "    print(f\"\\nTraining on {split_name}\")\n",
        "    \n",
        "    # Initialize model, criterion, optimizer for this split\n",
        "    model = ReturnPredictor().to(device)\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    \n",
        "    # Early stopping parameters\n",
        "    min_epochs = 4  # Train for at least 4 epochs\n",
        "    max_epochs = 10  # Train for at most 10 epochs\n",
        "    patience = 3  # Stop if no improvement for 3 consecutive epochs\n",
        "    min_delta = 0.001  # Minimum improvement threshold\n",
        "    \n",
        "    best_val_loss = float('inf')\n",
        "    epochs_no_improve = 0\n",
        "    best_model_state = None\n",
        "    \n",
        "    train_loader = loaders['train']\n",
        "    val_loader = loaders['val']\n",
        "    test_loader = loaders['test']\n",
        "    \n",
        "    # Train the model\n",
        "    total_training_time = 0\n",
        "    for epoch in range(max_epochs):\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        epoch_start_time = time.time()\n",
        "        \n",
        "        # Add progress bar to display training progress\n",
        "        train_progress = tqdm(train_loader, desc=f'Epoch {epoch+1}/{max_epochs} [Train]')\n",
        "        for batch_bert, batch_returns in train_progress:\n",
        "            batch_bert = batch_bert.to(device)\n",
        "            batch_returns = batch_returns.to(device)\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(batch_bert)\n",
        "            loss = criterion(outputs, batch_returns)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            train_loss += loss.item()\n",
        "            # Update progress bar with current loss\n",
        "            train_progress.set_postfix(loss=f'{loss.item():.4f}')\n",
        "        \n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        with torch.no_grad():\n",
        "            # Add progress bar to display validation progress\n",
        "            val_progress = tqdm(val_loader, desc=f'Epoch {epoch+1}/{max_epochs} [Val]')\n",
        "            for batch_bert, batch_returns in val_progress:\n",
        "                batch_bert = batch_bert.to(device)\n",
        "                batch_returns = batch_returns.to(device)\n",
        "                outputs = model(batch_bert)            \n",
        "                current_loss = criterion(outputs, batch_returns).item()\n",
        "                val_loss += current_loss\n",
        "                # Update progress bar with current loss\n",
        "                val_progress.set_postfix(loss=f'{current_loss:.4f}')\n",
        "        \n",
        "        epoch_end_time = time.time()\n",
        "        epoch_duration = epoch_end_time - epoch_start_time\n",
        "        total_training_time += epoch_duration\n",
        "        \n",
        "        train_loss = train_loss / len(train_loader)\n",
        "        val_loss = val_loss / len(val_loader)\n",
        "        \n",
        "        train_losses.append(train_loss)\n",
        "        val_losses.append(val_loss)\n",
        "        \n",
        "        print(f'Epoch [{epoch+1}/{max_epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Time: {epoch_duration:.2f}s')\n",
        "        \n",
        "        # Early stopping logic\n",
        "        if val_loss < best_val_loss - min_delta:\n",
        "            # Significant improvement\n",
        "            best_val_loss = val_loss\n",
        "            epochs_no_improve = 0\n",
        "            # Save best model state\n",
        "            best_model_state = model.state_dict().copy()\n",
        "        else:\n",
        "            # No significant improvement\n",
        "            epochs_no_improve += 1\n",
        "            \n",
        "        # Check if training should stop\n",
        "        if epoch + 1 >= min_epochs and epochs_no_improve >= patience:\n",
        "            print(f\"Early stopping triggered after {epoch+1} epochs. No improvement for {epochs_no_improve} epochs.\")\n",
        "            # Restore to best model state\n",
        "            if best_model_state is not None:\n",
        "                model.load_state_dict(best_model_state)\n",
        "            break\n",
        "    \n",
        "    avg_epoch_time = total_training_time / (epoch + 1)\n",
        "    print(f'Average epoch training time: {avg_epoch_time:.2f}s')\n",
        "    \n",
        "    # Test evaluation\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    predictions = []\n",
        "    actuals = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        # Add progress bar to display testing progress\n",
        "        test_progress = tqdm(test_loader, desc=f'Testing {split_name}')\n",
        "        for batch_bert, batch_returns in test_progress:\n",
        "            batch_bert = batch_bert.to(device)\n",
        "            batch_returns = batch_returns.to(device)\n",
        "            outputs = model(batch_bert)\n",
        "            current_loss = criterion(outputs, batch_returns).item()\n",
        "            test_loss += current_loss\n",
        "            \n",
        "            predictions.extend(outputs.cpu().numpy().flatten())\n",
        "            actuals.extend(batch_returns.cpu().numpy().flatten())\n",
        "            # Update progress bar with current loss\n",
        "            test_progress.set_postfix(loss=f'{current_loss:.4f}')\n",
        "    \n",
        "    test_loss = test_loss / len(test_loader)\n",
        "    \n",
        "    # Store results for this split\n",
        "    results[split_name] = {\n",
        "        'model': model,\n",
        "        'train_losses': train_losses,\n",
        "        'val_losses': val_losses,\n",
        "        'test_loss': test_loss,\n",
        "        'predictions': predictions,\n",
        "        'actuals': actuals,\n",
        "        'avg_epoch_time': avg_epoch_time, \n",
        "        'epochs_trained': epoch + 1\n",
        "    }\n",
        "    \n",
        "    print(f\"{split_name} Test Loss: {test_loss:.4f}, Epochs Trained: {epoch+1}\")\n",
        "\n",
        "print(\"\\nTraining completed for all splits.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_r2(y_true, y_pred, in_sample=True, benchmark=None):\n",
        "    if in_sample:\n",
        "        return 1 - (np.sum((y_true - y_pred) ** 2) / \n",
        "                    np.sum((y_true - np.mean(y_true)) ** 2))\n",
        "    else:\n",
        "        if benchmark is None:\n",
        "            raise ValueError(\"Benchmark must be provided for out-of-sample R-squared calculation.\")\n",
        "        return 1 - (np.sum((y_true - y_pred) ** 2) / \n",
        "                    np.sum((y_true - benchmark) ** 2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "split_1 Test Results:\n",
            "R² Score: -0.191368\n",
            "MSE: 0.000427\n",
            "\n",
            "split_2 Test Results:\n",
            "R² Score: -0.130437\n",
            "MSE: 0.000778\n",
            "\n",
            "split_3 Test Results:\n",
            "R² Score: -0.006831\n",
            "MSE: 0.000876\n",
            "\n",
            "split_4 Test Results:\n",
            "R² Score: -0.043816\n",
            "MSE: 0.001311\n",
            "\n",
            "split_5 Test Results:\n",
            "R² Score: -0.028990\n",
            "MSE: 0.001772\n",
            "\n",
            "split_6 Test Results:\n",
            "R² Score: -0.235239\n",
            "MSE: 0.000672\n",
            "\n",
            "split_7 Test Results:\n",
            "R² Score: -0.077370\n",
            "MSE: 0.000508\n",
            "\n",
            "split_8 Test Results:\n",
            "R² Score: -0.125900\n",
            "MSE: 0.000655\n",
            "\n",
            "split_9 Test Results:\n",
            "R² Score: -0.169601\n",
            "MSE: 0.001461\n",
            "\n",
            "split_10 Test Results:\n",
            "R² Score: -0.096404\n",
            "MSE: 0.000681\n",
            "\n",
            "split_11 Test Results:\n",
            "R² Score: -0.049841\n",
            "MSE: 0.000709\n",
            "\n",
            "split_12 Test Results:\n",
            "R² Score: -0.010872\n",
            "MSE: 0.004941\n",
            "\n",
            "split_13 Test Results:\n",
            "R² Score: -0.051972\n",
            "MSE: 0.001390\n",
            "\n",
            "split_14 Test Results:\n",
            "R² Score: -0.026567\n",
            "MSE: 0.001188\n",
            "\n",
            "split_15 Test Results:\n",
            "R² Score: -0.106183\n",
            "MSE: 0.000793\n",
            "\n",
            "split_16 Test Results:\n",
            "R² Score: -0.318277\n",
            "MSE: 0.000957\n",
            "\n",
            "Evaluation Metrics Summary for All Splits:\n",
            "       Split        R²       MSE\n",
            "0    split_1 -0.191368  0.000427\n",
            "1    split_2 -0.130437  0.000778\n",
            "2    split_3 -0.006831  0.000876\n",
            "3    split_4 -0.043816  0.001311\n",
            "4    split_5 -0.028990  0.001772\n",
            "5    split_6 -0.235239  0.000672\n",
            "6    split_7 -0.077370  0.000508\n",
            "7    split_8 -0.125900  0.000655\n",
            "8    split_9 -0.169601  0.001461\n",
            "9   split_10 -0.096404  0.000681\n",
            "10  split_11 -0.049841  0.000709\n",
            "11  split_12 -0.010872  0.004941\n",
            "12  split_13 -0.051972  0.001390\n",
            "13  split_14 -0.026567  0.001188\n",
            "14  split_15 -0.106183  0.000793\n",
            "15  split_16 -0.318277  0.000957\n"
          ]
        }
      ],
      "source": [
        "# Evaluate model performance across all splits\n",
        "evaluation_results = {}\n",
        "\n",
        "for split_name, split_data in results.items():\n",
        "    model = split_data['model']\n",
        "    test_loader = data_loaders[split_name]['test']\n",
        "    \n",
        "    # Evaluate model performance on the test set\n",
        "    model.eval()\n",
        "    test_predictions = []\n",
        "    test_actuals = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch_bert, batch_returns in test_loader:\n",
        "            batch_bert = batch_bert.to(device)\n",
        "            outputs = model(batch_bert)\n",
        "            test_predictions.extend(outputs.cpu().numpy())\n",
        "            test_actuals.extend(batch_returns.numpy())\n",
        "    \n",
        "    test_predictions = np.array(test_predictions)\n",
        "    test_actuals = np.array(test_actuals)\n",
        "    \n",
        "    # Calculate R² and MSE\n",
        "    r2 = calculate_r2(test_actuals, test_predictions, in_sample=False, benchmark=0)\n",
        "    mse = np.mean((test_predictions - test_actuals) ** 2)\n",
        "    \n",
        "    # Store evaluation results\n",
        "    evaluation_results[split_name] = {\n",
        "        'r2': r2,\n",
        "        'mse': mse\n",
        "    }\n",
        "    \n",
        "    # Print evaluation metrics\n",
        "    print(f\"\\n{split_name} Test Results:\")\n",
        "    print(f\"R² Score: {r2:.6f}\")\n",
        "    print(f\"MSE: {mse:.6f}\")\n",
        "\n",
        "# Create summary table of evaluation results\n",
        "import pandas as pd\n",
        "\n",
        "# Extract metrics for each split\n",
        "summary_data = []\n",
        "for split_name, metrics in evaluation_results.items():\n",
        "    summary_data.append({\n",
        "        'Split': split_name,\n",
        "        'R²': metrics['r2'],\n",
        "        'MSE': metrics['mse']\n",
        "    })\n",
        "\n",
        "# Create summary DataFrame\n",
        "summary_df = pd.DataFrame(summary_data)\n",
        "print(\"\\nEvaluation Metrics Summary for All Splits:\")\n",
        "print(summary_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "All predictions saved to news_w_sentiment_FineTuneBERT.csv, total records: 13696\n",
            "\n",
            "Data preview:\n",
            "   publication_datetime                                              title  \\\n",
            "0            2018-05-01                                Still Hot For Cocoa   \n",
            "22           2018-05-01                      T-Mobile Must Stay a Maverick   \n",
            "21           2018-05-01  Business News: Deal to Forge Biggest U.S. Refiner   \n",
            "20           2018-05-01  U.S. News: High Court to Heighten Class-Action...   \n",
            "19           2018-05-01             Walmart Pulls Back From U.K. Groceries   \n",
            "\n",
            "                                                 body tickers  sentiment  \n",
            "0   That compares with aroughly 7% gain for a Gold...     HSY   0.000272  \n",
            "22  The politically important sideshow will be whe...    TMUS   0.012052  \n",
            "21  Marathon's shares have nearly tripled since th...     MPC   0.002517  \n",
            "20  But rather than try to distribute that money a...   GOOGL  -0.001273  \n",
            "19  Walmart bought the U.K.'s third-largest grocer...     WMT   0.010260  \n"
          ]
        }
      ],
      "source": [
        "# Save test set predictions for each split\n",
        "all_test_dfs = []\n",
        "\n",
        "for split_name, split_data in results.items():\n",
        "    # Get corresponding test set data and predictions\n",
        "    test_set = rolling_splits[int(split_name.split('_')[1])-1]['test_set']\n",
        "    test_predictions = split_data['predictions']\n",
        "    \n",
        "    # Copy test data and add predictions as sentiment\n",
        "    test_df = test_set.copy()\n",
        "    test_df['sentiment'] = test_predictions\n",
        "    \n",
        "    # Keep only necessary columns\n",
        "    keep_columns = ['publication_datetime', 'title', 'body', 'tickers', 'sentiment']\n",
        "    test_df = test_df[keep_columns]\n",
        "    \n",
        "    # Add to the list\n",
        "    all_test_dfs.append(test_df)\n",
        "\n",
        "# Merge all test data\n",
        "if all_test_dfs:\n",
        "    all_test_predictions = pd.concat(all_test_dfs, ignore_index=True)\n",
        "    \n",
        "    # Sort by date\n",
        "    all_test_predictions['publication_datetime'] = pd.to_datetime(all_test_predictions['publication_datetime'])\n",
        "    all_test_predictions = all_test_predictions.sort_values('publication_datetime')\n",
        "    \n",
        "    # Save the merged predictions\n",
        "    all_test_predictions.to_csv(\"news_w_sentiment_FineTuneBERT.csv\", index=False)\n",
        "    print(f\"All predictions saved to news_w_sentiment_FineTuneBERT.csv, total records: {len(all_test_predictions)}\")\n",
        "    \n",
        "    # Display the first few rows to preview the results\n",
        "    print(\"\\nData preview:\")\n",
        "    print(all_test_predictions.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "All model results saved to 'fine-tuned_bert_model_results' directory\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create directory to save results\n",
        "os.makedirs(\"fine-tuned_bert_model_results\", exist_ok=True)\n",
        "\n",
        "# Save results for each split\n",
        "for split_name, split_result in results.items():\n",
        "    # Create subdirectory for each split\n",
        "    split_dir = os.path.join(\"fine-tuned_bert_model_results\", split_name)\n",
        "    os.makedirs(split_dir, exist_ok=True)\n",
        "    \n",
        "    # 1. Save model weights\n",
        "    model_path = os.path.join(split_dir, f\"{split_name}_model.pth\")\n",
        "    torch.save(split_result['model'].state_dict(), model_path)\n",
        "    \n",
        "    # 2. Save training and validation loss history\n",
        "    losses = {\n",
        "        'train_losses': split_result['train_losses'],\n",
        "        'val_losses': split_result['val_losses'],\n",
        "        'test_loss': split_result['test_loss']\n",
        "    }\n",
        "    \n",
        "    # Convert NumPy arrays to Python lists for JSON serialization\n",
        "    for key in losses:\n",
        "        if isinstance(losses[key], (np.ndarray, list)):\n",
        "            losses[key] = [float(x) for x in losses[key]]\n",
        "        elif isinstance(losses[key], (np.float32, np.float64)):\n",
        "            losses[key] = float(losses[key])\n",
        "    \n",
        "    # Save loss data\n",
        "    with open(os.path.join(split_dir, f\"{split_name}_losses.json\"), 'w') as f:\n",
        "        json.dump(losses, f, indent=4)\n",
        "    \n",
        "    # 3. Save prediction results\n",
        "    predictions_data = {\n",
        "        'predictions': [float(p) for p in split_result['predictions']],\n",
        "        'actuals': [float(a) for a in split_result['actuals']]\n",
        "    }\n",
        "    with open(os.path.join(split_dir, f\"{split_name}_predictions.json\"), 'w') as f:\n",
        "        json.dump(predictions_data, f, indent=4)\n",
        "    \n",
        "    # 4. Plot and save loss curves\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(split_result['train_losses'], label='Train Loss')\n",
        "    plt.plot(split_result['val_losses'], label='Validation Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title(f'{split_name} - Training and Validation Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.savefig(os.path.join(split_dir, f\"{split_name}_loss_curve.png\"))\n",
        "    plt.close()\n",
        "    \n",
        "    # 5. Plot prediction vs actual values scatter plot\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.scatter(split_result['actuals'], split_result['predictions'], alpha=0.5)\n",
        "    plt.plot([-0.1, 0.1], [-0.1, 0.1], 'r--')  # Ideal line\n",
        "    plt.xlabel('Actual Returns')\n",
        "    plt.ylabel('Predicted Returns')\n",
        "    plt.title(f'{split_name} - Prediction vs Actual')\n",
        "    plt.grid(True)\n",
        "    plt.savefig(os.path.join(split_dir, f\"{split_name}_prediction_scatter.png\"))\n",
        "    plt.close()\n",
        "\n",
        "# Create README file with explanation of results\n",
        "readme_content = \"\"\"# Financial News Sentiment Analysis Model Results\n",
        "\n",
        "## Model Structure\n",
        "- BERT model for extracting news text features\n",
        "- Fully connected layers for stock return prediction\n",
        "- Trained using MSE loss function and Adam optimizer\n",
        "\n",
        "## File Descriptions\n",
        "Each time window split contains the following files:\n",
        "- `{split_name}_model.pth`: Trained model weights\n",
        "- `{split_name}_losses.json`: Training and validation loss values\n",
        "- `{split_name}_predictions.json`: Predictions and actual values on test set\n",
        "- `{split_name}_loss_curve.png`: Loss curve chart\n",
        "- `{split_name}_prediction_scatter.png`: Prediction vs actual values scatter plot\n",
        "\n",
        "## How to Load and Use the Model\"\"\"\n",
        "\n",
        "with open(os.path.join(\"fine-tuned_bert_model_results\", \"README.md\"), 'w') as f:\n",
        "    f.write(readme_content)\n",
        "\n",
        "print(\"All model results saved to 'fine-tuned_bert_model_results' directory\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Fine-tune FinBERT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading FinBERT model and tokenizer...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at yiyanghkust/finbert-pretrain and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30873, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSdpaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Import additional required libraries\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from torch.optim import AdamW\n",
        "import torch.nn as nn\n",
        "import time\n",
        "from tqdm import tqdm \n",
        "\n",
        "# Load FinBERT model and tokenizer\n",
        "print(\"Loading FinBERT model and tokenizer...\")\n",
        "finbert_tokenizer = BertTokenizer.from_pretrained('yiyanghkust/finbert-pretrain')\n",
        "finbert_model = BertForSequenceClassification.from_pretrained('yiyanghkust/finbert-pretrain')\n",
        "finbert_model.eval()  # Set to evaluation mode\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import numpy as np\n",
        "\n",
        "class FinBERTNewsDataset(Dataset):\n",
        "    def __init__(self, news_data, price_data, tokenizer, finbert_model, max_length=512, device=None):\n",
        "        self.news_data = news_data\n",
        "        self.price_data = price_data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.finbert_model = finbert_model\n",
        "        self.max_length = max_length\n",
        "        self.device = device or torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        \n",
        "        # Create a mapping of (date, ticker) to daily return\n",
        "        self.return_map = price_data.set_index(['Date', 'ticker'])['daily_return'].to_dict()\n",
        "        \n",
        "        # Pre-compute FinBERT embeddings for all articles\n",
        "        self.embeddings = []\n",
        "        \n",
        "        print(\"Computing FinBERT embeddings for all articles...\")\n",
        "        with torch.no_grad():\n",
        "            for idx in range(len(news_data)):\n",
        "                if idx % 1000 == 0:\n",
        "                    print(f\"Processing article {idx}/{len(news_data)}\")\n",
        "                    \n",
        "                text = news_data.iloc[idx]['body']\n",
        "                inputs = self.tokenizer(text, \n",
        "                                        return_tensors='pt',\n",
        "                                        max_length=self.max_length,\n",
        "                                        padding='max_length',\n",
        "                                        truncation=True)\n",
        "                \n",
        "                # Move inputs to the same device as the model\n",
        "                inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
        "                \n",
        "                outputs = self.finbert_model(**inputs, output_hidden_states=True)\n",
        "                # Use [CLS] token embedding from the last hidden state as document embedding\n",
        "                embedding = outputs.hidden_states[-1][:, 0, :].squeeze().cpu().numpy()\n",
        "                self.embeddings.append(embedding)\n",
        "                \n",
        "        self.embeddings = np.array(self.embeddings)\n",
        "        print(\"Finished computing embeddings.\")\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.news_data)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        news_row = self.news_data.iloc[idx]\n",
        "        date = news_row['publication_datetime']\n",
        "        ticker = news_row['tickers']\n",
        "        \n",
        "        # Get the next trading day's return\n",
        "        next_trading_days = self.price_data[\n",
        "            (self.price_data['Date'] >= date) & \n",
        "            (self.price_data['ticker'] == ticker)\n",
        "        ]['Date'].unique()\n",
        "        \n",
        "        if len(next_trading_days) > 0:\n",
        "            next_date = next_trading_days[0]\n",
        "            daily_return = self.return_map.get((next_date, ticker), 0)\n",
        "        else:\n",
        "            daily_return = 0\n",
        "            \n",
        "        # Get pre-computed FinBERT embedding\n",
        "        finbert_embedding = self.embeddings[idx]\n",
        "        \n",
        "        return torch.FloatTensor(finbert_embedding), torch.FloatTensor([daily_return])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the neural network for FinBERT\n",
        "class FinBERTReturnPredictor(nn.Module):\n",
        "    def __init__(self, input_size=768, hidden_size=256):  # FinBERT base also has 768 dimensions\n",
        "        super(FinBERTReturnPredictor, self).__init__()\n",
        "        self.network = nn.Sequential(\n",
        "            nn.Linear(input_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(hidden_size, hidden_size // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(hidden_size // 2, 1)\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.network(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating datasets for rolling splits...\n",
            "Creating split 1/16...\n",
            "Computing FinBERT embeddings for all articles...\n",
            "Processing article 0/5015\n",
            "Processing article 1000/5015\n",
            "Processing article 2000/5015\n",
            "Processing article 3000/5015\n",
            "Processing article 4000/5015\n",
            "Processing article 5000/5015\n",
            "Finished computing embeddings.\n",
            "Computing FinBERT embeddings for all articles...\n",
            "Processing article 0/1839\n",
            "Processing article 1000/1839\n",
            "Finished computing embeddings.\n",
            "Computing FinBERT embeddings for all articles...\n",
            "Processing article 0/902\n",
            "Finished computing embeddings.\n",
            "Creating split 2/16...\n",
            "Computing FinBERT embeddings for all articles...\n",
            "Processing article 0/5103\n",
            "Processing article 1000/5103\n",
            "Processing article 2000/5103\n",
            "Processing article 3000/5103\n",
            "Processing article 4000/5103\n",
            "Processing article 5000/5103\n",
            "Finished computing embeddings.\n",
            "Computing FinBERT embeddings for all articles...\n",
            "Processing article 0/1916\n",
            "Processing article 1000/1916\n",
            "Finished computing embeddings.\n",
            "Computing FinBERT embeddings for all articles...\n",
            "Processing article 0/848\n",
            "Finished computing embeddings.\n",
            "Creating split 3/16...\n",
            "Computing FinBERT embeddings for all articles...\n",
            "Processing article 0/5308\n",
            "Processing article 1000/5308\n",
            "Processing article 2000/5308\n",
            "Processing article 3000/5308\n",
            "Processing article 4000/5308\n",
            "Processing article 5000/5308\n",
            "Finished computing embeddings.\n",
            "Computing FinBERT embeddings for all articles...\n",
            "Processing article 0/1750\n",
            "Processing article 1000/1750\n",
            "Finished computing embeddings.\n",
            "Computing FinBERT embeddings for all articles...\n",
            "Processing article 0/884\n",
            "Finished computing embeddings.\n",
            "Creating split 4/16...\n",
            "Computing FinBERT embeddings for all articles...\n",
            "Processing article 0/5365\n",
            "Processing article 1000/5365\n",
            "Processing article 2000/5365\n",
            "Processing article 3000/5365\n",
            "Processing article 4000/5365\n",
            "Processing article 5000/5365\n",
            "Finished computing embeddings.\n",
            "Computing FinBERT embeddings for all articles...\n",
            "Processing article 0/1732\n",
            "Processing article 1000/1732\n",
            "Finished computing embeddings.\n",
            "Computing FinBERT embeddings for all articles...\n",
            "Processing article 0/832\n",
            "Finished computing embeddings.\n",
            "Creating split 5/16...\n",
            "Computing FinBERT embeddings for all articles...\n",
            "Processing article 0/5350\n",
            "Processing article 1000/5350\n",
            "Processing article 2000/5350\n",
            "Processing article 3000/5350\n",
            "Processing article 4000/5350\n",
            "Processing article 5000/5350\n",
            "Finished computing embeddings.\n",
            "Computing FinBERT embeddings for all articles...\n",
            "Processing article 0/1716\n",
            "Processing article 1000/1716\n",
            "Finished computing embeddings.\n",
            "Computing FinBERT embeddings for all articles...\n",
            "Processing article 0/862\n",
            "Finished computing embeddings.\n",
            "Creating split 6/16...\n",
            "Computing FinBERT embeddings for all articles...\n",
            "Processing article 0/5334\n",
            "Processing article 1000/5334\n",
            "Processing article 2000/5334\n",
            "Processing article 3000/5334\n",
            "Processing article 4000/5334\n",
            "Processing article 5000/5334\n",
            "Finished computing embeddings.\n",
            "Computing FinBERT embeddings for all articles...\n",
            "Processing article 0/1694\n",
            "Processing article 1000/1694\n",
            "Finished computing embeddings.\n",
            "Computing FinBERT embeddings for all articles...\n",
            "Processing article 0/949\n",
            "Finished computing embeddings.\n",
            "Creating split 7/16...\n",
            "Computing FinBERT embeddings for all articles...\n",
            "Processing article 0/5305\n",
            "Processing article 1000/5305\n",
            "Processing article 2000/5305\n",
            "Processing article 3000/5305\n",
            "Processing article 4000/5305\n",
            "Processing article 5000/5305\n",
            "Finished computing embeddings.\n",
            "Computing FinBERT embeddings for all articles...\n",
            "Processing article 0/1811\n",
            "Processing article 1000/1811\n",
            "Finished computing embeddings.\n",
            "Computing FinBERT embeddings for all articles...\n",
            "Processing article 0/926\n",
            "Finished computing embeddings.\n",
            "Creating split 8/16...\n",
            "Computing FinBERT embeddings for all articles...\n",
            "Processing article 0/5342\n",
            "Processing article 1000/5342\n",
            "Processing article 2000/5342\n",
            "Processing article 3000/5342\n",
            "Processing article 4000/5342\n",
            "Processing article 5000/5342\n",
            "Finished computing embeddings.\n",
            "Computing FinBERT embeddings for all articles...\n",
            "Processing article 0/1875\n",
            "Processing article 1000/1875\n",
            "Finished computing embeddings.\n",
            "Computing FinBERT embeddings for all articles...\n",
            "Processing article 0/929\n",
            "Finished computing embeddings.\n",
            "Creating split 9/16...\n",
            "Computing FinBERT embeddings for all articles...\n",
            "Processing article 0/5277\n",
            "Processing article 1000/5277\n",
            "Processing article 2000/5277\n",
            "Processing article 3000/5277\n",
            "Processing article 4000/5277\n",
            "Processing article 5000/5277\n",
            "Finished computing embeddings.\n",
            "Computing FinBERT embeddings for all articles...\n",
            "Processing article 0/1855\n",
            "Processing article 1000/1855\n",
            "Finished computing embeddings.\n",
            "Computing FinBERT embeddings for all articles...\n",
            "Processing article 0/923\n",
            "Finished computing embeddings.\n",
            "Creating split 10/16...\n",
            "Computing FinBERT embeddings for all articles...\n",
            "Processing article 0/5301\n",
            "Processing article 1000/5301\n",
            "Processing article 2000/5301\n",
            "Processing article 3000/5301\n",
            "Processing article 4000/5301\n",
            "Processing article 5000/5301\n",
            "Finished computing embeddings.\n",
            "Computing FinBERT embeddings for all articles...\n",
            "Processing article 0/1852\n",
            "Processing article 1000/1852\n",
            "Finished computing embeddings.\n",
            "Computing FinBERT embeddings for all articles...\n",
            "Processing article 0/749\n",
            "Finished computing embeddings.\n",
            "Creating split 11/16...\n",
            "Computing FinBERT embeddings for all articles...\n",
            "Processing article 0/5382\n",
            "Processing article 1000/5382\n",
            "Processing article 2000/5382\n",
            "Processing article 3000/5382\n",
            "Processing article 4000/5382\n",
            "Processing article 5000/5382\n",
            "Finished computing embeddings.\n",
            "Computing FinBERT embeddings for all articles...\n",
            "Processing article 0/1672\n",
            "Processing article 1000/1672\n",
            "Finished computing embeddings.\n",
            "Computing FinBERT embeddings for all articles...\n",
            "Processing article 0/848\n",
            "Finished computing embeddings.\n",
            "Creating split 12/16...\n",
            "Computing FinBERT embeddings for all articles...\n",
            "Processing article 0/5421\n",
            "Processing article 1000/5421\n",
            "Processing article 2000/5421\n",
            "Processing article 3000/5421\n",
            "Processing article 4000/5421\n",
            "Processing article 5000/5421\n",
            "Finished computing embeddings.\n",
            "Computing FinBERT embeddings for all articles...\n",
            "Processing article 0/1597\n",
            "Processing article 1000/1597\n",
            "Finished computing embeddings.\n",
            "Computing FinBERT embeddings for all articles...\n",
            "Processing article 0/810\n",
            "Finished computing embeddings.\n",
            "Creating split 13/16...\n",
            "Computing FinBERT embeddings for all articles...\n",
            "Processing article 0/5338\n",
            "Processing article 1000/5338\n",
            "Processing article 2000/5338\n",
            "Processing article 3000/5338\n",
            "Processing article 4000/5338\n",
            "Processing article 5000/5338\n",
            "Finished computing embeddings.\n",
            "Computing FinBERT embeddings for all articles...\n",
            "Processing article 0/1658\n",
            "Processing article 1000/1658\n",
            "Finished computing embeddings.\n",
            "Computing FinBERT embeddings for all articles...\n",
            "Processing article 0/723\n",
            "Finished computing embeddings.\n",
            "Creating split 14/16...\n",
            "Computing FinBERT embeddings for all articles...\n",
            "Processing article 0/5324\n",
            "Processing article 1000/5324\n",
            "Processing article 2000/5324\n",
            "Processing article 3000/5324\n",
            "Processing article 4000/5324\n",
            "Processing article 5000/5324\n",
            "Finished computing embeddings.\n",
            "Computing FinBERT embeddings for all articles...\n",
            "Processing article 0/1533\n",
            "Processing article 1000/1533\n",
            "Finished computing embeddings.\n",
            "Computing FinBERT embeddings for all articles...\n",
            "Processing article 0/855\n",
            "Finished computing embeddings.\n",
            "Creating split 15/16...\n",
            "Computing FinBERT embeddings for all articles...\n",
            "Processing article 0/5185\n",
            "Processing article 1000/5185\n",
            "Processing article 2000/5185\n",
            "Processing article 3000/5185\n",
            "Processing article 4000/5185\n",
            "Processing article 5000/5185\n",
            "Finished computing embeddings.\n",
            "Computing FinBERT embeddings for all articles...\n",
            "Processing article 0/1578\n",
            "Processing article 1000/1578\n",
            "Finished computing embeddings.\n",
            "Computing FinBERT embeddings for all articles...\n",
            "Processing article 0/866\n",
            "Finished computing embeddings.\n",
            "Creating split 16/16...\n",
            "Computing FinBERT embeddings for all articles...\n",
            "Processing article 0/4982\n",
            "Processing article 1000/4982\n",
            "Processing article 2000/4982\n",
            "Processing article 3000/4982\n",
            "Processing article 4000/4982\n",
            "Finished computing embeddings.\n",
            "Computing FinBERT embeddings for all articles...\n",
            "Processing article 0/1721\n",
            "Processing article 1000/1721\n",
            "Finished computing embeddings.\n",
            "Computing FinBERT embeddings for all articles...\n",
            "Processing article 0/790\n",
            "Finished computing embeddings.\n",
            "Created 16 dataset splits\n"
          ]
        }
      ],
      "source": [
        "# Set random seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Move FinBERT model to GPU if available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "finbert_model.to(device)\n",
        "\n",
        "print(\"Creating datasets for rolling splits...\")\n",
        "# Create datasets based on the rolling splits defined earlier\n",
        "finbert_datasets = {}\n",
        "\n",
        "for i, split in enumerate(rolling_splits):\n",
        "    print(f\"Creating split {i+1}/{len(rolling_splits)}...\")\n",
        "    \n",
        "    # Create train dataset for this split\n",
        "    train_dataset_i = FinBERTNewsDataset(\n",
        "        split['train_set'],\n",
        "        price_df,\n",
        "        finbert_tokenizer,\n",
        "        finbert_model,\n",
        "        device=device  # 传入设备参数\n",
        "    )\n",
        "    \n",
        "    # Create validation dataset for this split\n",
        "    val_dataset_i = FinBERTNewsDataset(\n",
        "        split['val_set'],\n",
        "        price_df,\n",
        "        finbert_tokenizer,\n",
        "        finbert_model,\n",
        "        device=device  # 传入设备参数\n",
        "    )\n",
        "    \n",
        "    # Create test dataset for this split\n",
        "    test_dataset_i = FinBERTNewsDataset(\n",
        "        split['test_set'],\n",
        "        price_df,\n",
        "        finbert_tokenizer,\n",
        "        finbert_model,\n",
        "        device=device  # 传入设备参数\n",
        "    )\n",
        "    \n",
        "    # Store datasets for this split\n",
        "    finbert_datasets[f'split_{i+1}'] = {\n",
        "        'train': train_dataset_i,\n",
        "        'val': val_dataset_i,\n",
        "        'test': test_dataset_i\n",
        "    }\n",
        "\n",
        "print(f\"Created {len(finbert_datasets)} dataset splits\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Created data loaders for 16 splits\n"
          ]
        }
      ],
      "source": [
        "# Create data loaders for each split\n",
        "finbert_data_loaders = {}\n",
        "\n",
        "for split_name, split_datasets in finbert_datasets.items():\n",
        "    train_loader = DataLoader(split_datasets['train'], batch_size=32, shuffle=True)\n",
        "    val_loader = DataLoader(split_datasets['val'], batch_size=32)\n",
        "    test_loader = DataLoader(split_datasets['test'], batch_size=32)\n",
        "    \n",
        "    finbert_data_loaders[split_name] = {\n",
        "        'train': train_loader,\n",
        "        'val': val_loader,\n",
        "        'test': test_loader\n",
        "    }\n",
        "\n",
        "print(f\"Created data loaders for {len(finbert_data_loaders)} splits\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training on split_1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/10 [Train]: 100%|██████████| 157/157 [01:25<00:00,  1.84it/s, loss=0.0019]\n",
            "Epoch 1/10 [Val]: 100%|██████████| 58/58 [00:31<00:00,  1.86it/s, loss=0.0011]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/10], Train Loss: 0.0045, Val Loss: 0.0008, Time: 116.28s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/10 [Train]: 100%|██████████| 157/157 [01:25<00:00,  1.83it/s, loss=0.0028]\n",
            "Epoch 2/10 [Val]: 100%|██████████| 58/58 [00:31<00:00,  1.86it/s, loss=0.0012]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/10], Train Loss: 0.0016, Val Loss: 0.0007, Time: 116.88s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/10 [Train]: 100%|██████████| 157/157 [01:26<00:00,  1.82it/s, loss=0.0006]\n",
            "Epoch 3/10 [Val]: 100%|██████████| 58/58 [00:31<00:00,  1.86it/s, loss=0.0012]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [3/10], Train Loss: 0.0010, Val Loss: 0.0006, Time: 117.41s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/10 [Train]: 100%|██████████| 157/157 [01:26<00:00,  1.82it/s, loss=0.0005]\n",
            "Epoch 4/10 [Val]: 100%|██████████| 58/58 [00:31<00:00,  1.86it/s, loss=0.0012]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [4/10], Train Loss: 0.0008, Val Loss: 0.0007, Time: 117.34s\n",
            "Early stopping triggered after 4 epochs. No improvement for 3 epochs.\n",
            "Average epoch training time: 116.98s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Testing split_1: 100%|██████████| 29/29 [00:15<00:00,  1.89it/s, loss=0.0001]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "split_1 Test Loss: 0.0004, Epochs Trained: 4\n",
            "\n",
            "Training on split_2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/10 [Train]: 100%|██████████| 160/160 [01:27<00:00,  1.83it/s, loss=0.0010]\n",
            "Epoch 1/10 [Val]: 100%|██████████| 60/60 [00:32<00:00,  1.84it/s, loss=0.0005]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/10], Train Loss: 0.0024, Val Loss: 0.0006, Time: 120.20s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/10 [Train]: 100%|██████████| 160/160 [01:27<00:00,  1.83it/s, loss=0.0011]\n",
            "Epoch 2/10 [Val]: 100%|██████████| 60/60 [00:32<00:00,  1.84it/s, loss=0.0006]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/10], Train Loss: 0.0009, Val Loss: 0.0005, Time: 120.04s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/10 [Train]: 100%|██████████| 160/160 [01:28<00:00,  1.82it/s, loss=0.0005]\n",
            "Epoch 3/10 [Val]: 100%|██████████| 60/60 [00:32<00:00,  1.85it/s, loss=0.0005]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [3/10], Train Loss: 0.0007, Val Loss: 0.0005, Time: 120.57s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/10 [Train]: 100%|██████████| 160/160 [01:28<00:00,  1.82it/s, loss=0.0006]\n",
            "Epoch 4/10 [Val]: 100%|██████████| 60/60 [00:32<00:00,  1.84it/s, loss=0.0006]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [4/10], Train Loss: 0.0006, Val Loss: 0.0005, Time: 120.59s\n",
            "Early stopping triggered after 4 epochs. No improvement for 3 epochs.\n",
            "Average epoch training time: 120.35s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Testing split_2: 100%|██████████| 27/27 [00:14<00:00,  1.88it/s, loss=0.0002]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "split_2 Test Loss: 0.0007, Epochs Trained: 4\n",
            "\n",
            "Training on split_3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/10 [Train]: 100%|██████████| 166/166 [01:31<00:00,  1.81it/s, loss=0.0013]\n",
            "Epoch 1/10 [Val]: 100%|██████████| 55/55 [00:29<00:00,  1.85it/s, loss=0.0009]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/10], Train Loss: 0.0040, Val Loss: 0.0008, Time: 121.39s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/10 [Train]: 100%|██████████| 166/166 [01:31<00:00,  1.81it/s, loss=0.0013]\n",
            "Epoch 2/10 [Val]: 100%|██████████| 55/55 [00:29<00:00,  1.85it/s, loss=0.0006]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/10], Train Loss: 0.0016, Val Loss: 0.0007, Time: 121.41s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/10 [Train]: 100%|██████████| 166/166 [01:31<00:00,  1.81it/s, loss=0.0009]\n",
            "Epoch 3/10 [Val]: 100%|██████████| 55/55 [00:29<00:00,  1.85it/s, loss=0.0004]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [3/10], Train Loss: 0.0010, Val Loss: 0.0007, Time: 121.23s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/10 [Train]: 100%|██████████| 166/166 [01:32<00:00,  1.79it/s, loss=0.0011]\n",
            "Epoch 4/10 [Val]: 100%|██████████| 55/55 [00:30<00:00,  1.81it/s, loss=0.0004]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [4/10], Train Loss: 0.0008, Val Loss: 0.0007, Time: 123.16s\n",
            "Early stopping triggered after 4 epochs. No improvement for 3 epochs.\n",
            "Average epoch training time: 121.80s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Testing split_3: 100%|██████████| 28/28 [00:15<00:00,  1.83it/s, loss=0.0010]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "split_3 Test Loss: 0.0010, Epochs Trained: 4\n",
            "\n",
            "Training on split_4\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/10 [Train]: 100%|██████████| 168/168 [01:33<00:00,  1.79it/s, loss=0.0015]\n",
            "Epoch 1/10 [Val]: 100%|██████████| 55/55 [00:29<00:00,  1.84it/s, loss=0.0007]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/10], Train Loss: 0.0031, Val Loss: 0.0010, Time: 123.71s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/10 [Train]: 100%|██████████| 168/168 [01:33<00:00,  1.79it/s, loss=0.0014]\n",
            "Epoch 2/10 [Val]: 100%|██████████| 55/55 [00:30<00:00,  1.83it/s, loss=0.0006]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/10], Train Loss: 0.0013, Val Loss: 0.0010, Time: 123.76s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/10 [Train]: 100%|██████████| 168/168 [01:33<00:00,  1.80it/s, loss=0.0006]\n",
            "Epoch 3/10 [Val]: 100%|██████████| 55/55 [00:29<00:00,  1.86it/s, loss=0.0007]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [3/10], Train Loss: 0.0009, Val Loss: 0.0010, Time: 123.10s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/10 [Train]: 100%|██████████| 168/168 [01:32<00:00,  1.82it/s, loss=0.0006]\n",
            "Epoch 4/10 [Val]: 100%|██████████| 55/55 [00:29<00:00,  1.86it/s, loss=0.0007]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [4/10], Train Loss: 0.0008, Val Loss: 0.0010, Time: 121.92s\n",
            "Early stopping triggered after 4 epochs. No improvement for 3 epochs.\n",
            "Average epoch training time: 123.12s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Testing split_4: 100%|██████████| 26/26 [00:14<00:00,  1.84it/s, loss=0.0003]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "split_4 Test Loss: 0.0016, Epochs Trained: 4\n",
            "\n",
            "Training on split_5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/10 [Train]: 100%|██████████| 168/168 [01:31<00:00,  1.83it/s, loss=0.0014]\n",
            "Epoch 1/10 [Val]: 100%|██████████| 54/54 [00:29<00:00,  1.85it/s, loss=0.0005]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/10], Train Loss: 0.0029, Val Loss: 0.0012, Time: 121.13s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/10 [Train]: 100%|██████████| 168/168 [01:31<00:00,  1.83it/s, loss=0.0006]\n",
            "Epoch 2/10 [Val]: 100%|██████████| 54/54 [00:29<00:00,  1.85it/s, loss=0.0004]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/10], Train Loss: 0.0012, Val Loss: 0.0012, Time: 121.08s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/10 [Train]: 100%|██████████| 168/168 [01:31<00:00,  1.83it/s, loss=0.0002]\n",
            "Epoch 3/10 [Val]: 100%|██████████| 54/54 [00:29<00:00,  1.85it/s, loss=0.0003]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [3/10], Train Loss: 0.0009, Val Loss: 0.0012, Time: 121.09s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/10 [Train]: 100%|██████████| 168/168 [01:31<00:00,  1.83it/s, loss=0.0007]\n",
            "Epoch 4/10 [Val]: 100%|██████████| 54/54 [00:29<00:00,  1.85it/s, loss=0.0003]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [4/10], Train Loss: 0.0008, Val Loss: 0.0012, Time: 121.15s\n",
            "Early stopping triggered after 4 epochs. No improvement for 3 epochs.\n",
            "Average epoch training time: 121.11s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Testing split_5: 100%|██████████| 27/27 [00:14<00:00,  1.84it/s, loss=0.0016]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "split_5 Test Loss: 0.0019, Epochs Trained: 4\n",
            "\n",
            "Training on split_6\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/10 [Train]: 100%|██████████| 167/167 [01:31<00:00,  1.82it/s, loss=0.0020]\n",
            "Epoch 1/10 [Val]: 100%|██████████| 53/53 [00:28<00:00,  1.84it/s, loss=0.0014]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/10], Train Loss: 0.0033, Val Loss: 0.0016, Time: 120.65s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/10 [Train]: 100%|██████████| 167/167 [01:31<00:00,  1.82it/s, loss=0.0006]\n",
            "Epoch 2/10 [Val]: 100%|██████████| 53/53 [00:28<00:00,  1.83it/s, loss=0.0015]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/10], Train Loss: 0.0014, Val Loss: 0.0016, Time: 120.70s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/10 [Train]: 100%|██████████| 167/167 [01:32<00:00,  1.80it/s, loss=0.0006]\n",
            "Epoch 3/10 [Val]: 100%|██████████| 53/53 [00:28<00:00,  1.84it/s, loss=0.0014]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [3/10], Train Loss: 0.0010, Val Loss: 0.0016, Time: 121.71s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/10 [Train]: 100%|██████████| 167/167 [01:32<00:00,  1.81it/s, loss=0.0005]\n",
            "Epoch 4/10 [Val]: 100%|██████████| 53/53 [00:28<00:00,  1.84it/s, loss=0.0014]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [4/10], Train Loss: 0.0008, Val Loss: 0.0016, Time: 121.25s\n",
            "Early stopping triggered after 4 epochs. No improvement for 3 epochs.\n",
            "Average epoch training time: 121.08s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Testing split_6: 100%|██████████| 30/30 [00:16<00:00,  1.86it/s, loss=0.0012]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "split_6 Test Loss: 0.0006, Epochs Trained: 4\n",
            "\n",
            "Training on split_7\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/10 [Train]: 100%|██████████| 166/166 [01:32<00:00,  1.80it/s, loss=0.0021]\n",
            "Epoch 1/10 [Val]: 100%|██████████| 57/57 [00:30<00:00,  1.85it/s, loss=0.0012]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/10], Train Loss: 0.0026, Val Loss: 0.0012, Time: 122.83s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/10 [Train]: 100%|██████████| 166/166 [01:31<00:00,  1.81it/s, loss=0.0020]\n",
            "Epoch 2/10 [Val]: 100%|██████████| 57/57 [00:30<00:00,  1.85it/s, loss=0.0010]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/10], Train Loss: 0.0013, Val Loss: 0.0013, Time: 122.55s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/10 [Train]: 100%|██████████| 166/166 [01:31<00:00,  1.81it/s, loss=0.0005]\n",
            "Epoch 3/10 [Val]: 100%|██████████| 57/57 [00:30<00:00,  1.85it/s, loss=0.0010]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [3/10], Train Loss: 0.0011, Val Loss: 0.0013, Time: 122.71s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/10 [Train]: 100%|██████████| 166/166 [01:31<00:00,  1.81it/s, loss=0.0008]\n",
            "Epoch 4/10 [Val]: 100%|██████████| 57/57 [00:30<00:00,  1.85it/s, loss=0.0009]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [4/10], Train Loss: 0.0010, Val Loss: 0.0012, Time: 122.82s\n",
            "Early stopping triggered after 4 epochs. No improvement for 3 epochs.\n",
            "Average epoch training time: 122.73s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Testing split_7: 100%|██████████| 29/29 [00:15<00:00,  1.84it/s, loss=0.0007]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "split_7 Test Loss: 0.0006, Epochs Trained: 4\n",
            "\n",
            "Training on split_8\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/10 [Train]: 100%|██████████| 167/167 [01:32<00:00,  1.80it/s, loss=0.0013]\n",
            "Epoch 1/10 [Val]: 100%|██████████| 59/59 [00:32<00:00,  1.84it/s, loss=0.0004]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/10], Train Loss: 0.0033, Val Loss: 0.0006, Time: 124.86s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/10 [Train]: 100%|██████████| 167/167 [01:32<00:00,  1.80it/s, loss=0.0013]\n",
            "Epoch 2/10 [Val]: 100%|██████████| 59/59 [00:31<00:00,  1.85it/s, loss=0.0003]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/10], Train Loss: 0.0017, Val Loss: 0.0006, Time: 124.56s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/10 [Train]: 100%|██████████| 167/167 [01:32<00:00,  1.80it/s, loss=0.0013]\n",
            "Epoch 3/10 [Val]: 100%|██████████| 59/59 [00:31<00:00,  1.85it/s, loss=0.0002]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [3/10], Train Loss: 0.0014, Val Loss: 0.0006, Time: 124.65s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/10 [Train]: 100%|██████████| 167/167 [01:33<00:00,  1.79it/s, loss=0.0007]\n",
            "Epoch 4/10 [Val]: 100%|██████████| 59/59 [00:31<00:00,  1.85it/s, loss=0.0002]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [4/10], Train Loss: 0.0012, Val Loss: 0.0006, Time: 125.31s\n",
            "Early stopping triggered after 4 epochs. No improvement for 3 epochs.\n",
            "Average epoch training time: 124.85s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Testing split_8: 100%|██████████| 30/30 [00:15<00:00,  1.89it/s, loss=0.0003]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "split_8 Test Loss: 0.0007, Epochs Trained: 4\n",
            "\n",
            "Training on split_9\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/10 [Train]: 100%|██████████| 165/165 [01:31<00:00,  1.81it/s, loss=0.0016]\n",
            "Epoch 1/10 [Val]: 100%|██████████| 58/58 [00:31<00:00,  1.84it/s, loss=0.0007]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/10], Train Loss: 0.0031, Val Loss: 0.0006, Time: 122.93s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/10 [Train]: 100%|██████████| 165/165 [01:31<00:00,  1.80it/s, loss=0.0018]\n",
            "Epoch 2/10 [Val]: 100%|██████████| 58/58 [00:31<00:00,  1.84it/s, loss=0.0007]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/10], Train Loss: 0.0017, Val Loss: 0.0006, Time: 122.98s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/10 [Train]: 100%|██████████| 165/165 [01:31<00:00,  1.81it/s, loss=0.0009]\n",
            "Epoch 3/10 [Val]: 100%|██████████| 58/58 [00:31<00:00,  1.84it/s, loss=0.0007]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [3/10], Train Loss: 0.0014, Val Loss: 0.0007, Time: 122.86s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/10 [Train]: 100%|██████████| 165/165 [01:31<00:00,  1.80it/s, loss=0.0008]\n",
            "Epoch 4/10 [Val]: 100%|██████████| 58/58 [00:31<00:00,  1.84it/s, loss=0.0006]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [4/10], Train Loss: 0.0012, Val Loss: 0.0007, Time: 123.03s\n",
            "Early stopping triggered after 4 epochs. No improvement for 3 epochs.\n",
            "Average epoch training time: 122.95s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Testing split_9: 100%|██████████| 29/29 [00:15<00:00,  1.85it/s, loss=0.0002]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "split_9 Test Loss: 0.0013, Epochs Trained: 4\n",
            "\n",
            "Training on split_10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/10 [Train]: 100%|██████████| 166/166 [01:32<00:00,  1.79it/s, loss=0.0011]\n",
            "Epoch 1/10 [Val]: 100%|██████████| 58/58 [00:31<00:00,  1.84it/s, loss=0.0003]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/10], Train Loss: 0.0038, Val Loss: 0.0011, Time: 124.02s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/10 [Train]: 100%|██████████| 166/166 [01:31<00:00,  1.81it/s, loss=0.0006]\n",
            "Epoch 2/10 [Val]: 100%|██████████| 58/58 [00:31<00:00,  1.84it/s, loss=0.0004]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/10], Train Loss: 0.0017, Val Loss: 0.0010, Time: 123.40s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/10 [Train]: 100%|██████████| 166/166 [01:32<00:00,  1.80it/s, loss=0.0005]\n",
            "Epoch 3/10 [Val]: 100%|██████████| 58/58 [00:31<00:00,  1.84it/s, loss=0.0004]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [3/10], Train Loss: 0.0013, Val Loss: 0.0010, Time: 123.64s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/10 [Train]: 100%|██████████| 166/166 [01:31<00:00,  1.81it/s, loss=0.0007]\n",
            "Epoch 4/10 [Val]: 100%|██████████| 58/58 [00:31<00:00,  1.84it/s, loss=0.0004]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [4/10], Train Loss: 0.0011, Val Loss: 0.0010, Time: 123.32s\n",
            "Early stopping triggered after 4 epochs. No improvement for 3 epochs.\n",
            "Average epoch training time: 123.59s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Testing split_10: 100%|██████████| 24/24 [00:12<00:00,  1.88it/s, loss=0.0002]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "split_10 Test Loss: 0.0007, Epochs Trained: 4\n",
            "\n",
            "Training on split_11\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/10 [Train]: 100%|██████████| 169/169 [01:33<00:00,  1.80it/s, loss=0.0003]\n",
            "Epoch 1/10 [Val]: 100%|██████████| 53/53 [00:28<00:00,  1.86it/s, loss=0.0001]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/10], Train Loss: 0.0026, Val Loss: 0.0011, Time: 122.26s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/10 [Train]: 100%|██████████| 169/169 [01:33<00:00,  1.81it/s, loss=0.0004]\n",
            "Epoch 2/10 [Val]: 100%|██████████| 53/53 [00:28<00:00,  1.86it/s, loss=0.0002]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/10], Train Loss: 0.0015, Val Loss: 0.0011, Time: 122.07s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/10 [Train]: 100%|██████████| 169/169 [01:33<00:00,  1.81it/s, loss=0.0016]\n",
            "Epoch 3/10 [Val]: 100%|██████████| 53/53 [00:28<00:00,  1.86it/s, loss=0.0002]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [3/10], Train Loss: 0.0013, Val Loss: 0.0011, Time: 121.95s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/10 [Train]: 100%|██████████| 169/169 [01:33<00:00,  1.82it/s, loss=0.0022]\n",
            "Epoch 4/10 [Val]: 100%|██████████| 53/53 [00:28<00:00,  1.86it/s, loss=0.0003]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [4/10], Train Loss: 0.0012, Val Loss: 0.0011, Time: 121.47s\n",
            "Early stopping triggered after 4 epochs. No improvement for 3 epochs.\n",
            "Average epoch training time: 121.94s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Testing split_11: 100%|██████████| 27/27 [00:14<00:00,  1.87it/s, loss=0.0022]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "split_11 Test Loss: 0.0008, Epochs Trained: 4\n",
            "\n",
            "Training on split_12\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/10 [Train]: 100%|██████████| 170/170 [01:33<00:00,  1.81it/s, loss=0.0012]\n",
            "Epoch 1/10 [Val]: 100%|██████████| 50/50 [00:27<00:00,  1.84it/s, loss=0.0023]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/10], Train Loss: 0.0038, Val Loss: 0.0008, Time: 121.08s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/10 [Train]: 100%|██████████| 170/170 [01:33<00:00,  1.81it/s, loss=0.0006]\n",
            "Epoch 2/10 [Val]: 100%|██████████| 50/50 [00:27<00:00,  1.84it/s, loss=0.0021]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/10], Train Loss: 0.0019, Val Loss: 0.0008, Time: 120.99s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/10 [Train]: 100%|██████████| 170/170 [01:33<00:00,  1.81it/s, loss=0.0003]\n",
            "Epoch 3/10 [Val]: 100%|██████████| 50/50 [00:27<00:00,  1.84it/s, loss=0.0020]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [3/10], Train Loss: 0.0015, Val Loss: 0.0008, Time: 121.14s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/10 [Train]: 100%|██████████| 170/170 [01:34<00:00,  1.81it/s, loss=0.0005]\n",
            "Epoch 4/10 [Val]: 100%|██████████| 50/50 [00:27<00:00,  1.84it/s, loss=0.0020]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [4/10], Train Loss: 0.0013, Val Loss: 0.0008, Time: 121.21s\n",
            "Early stopping triggered after 4 epochs. No improvement for 3 epochs.\n",
            "Average epoch training time: 121.10s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Testing split_12: 100%|██████████| 26/26 [00:13<00:00,  1.88it/s, loss=0.0021]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "split_12 Test Loss: 0.0051, Epochs Trained: 4\n",
            "\n",
            "Training on split_13\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/10 [Train]: 100%|██████████| 167/167 [01:32<00:00,  1.81it/s, loss=0.0035]\n",
            "Epoch 1/10 [Val]: 100%|██████████| 52/52 [00:28<00:00,  1.84it/s, loss=0.0015]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/10], Train Loss: 0.0049, Val Loss: 0.0029, Time: 120.40s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/10 [Train]: 100%|██████████| 167/167 [01:32<00:00,  1.81it/s, loss=0.0018]\n",
            "Epoch 2/10 [Val]: 100%|██████████| 52/52 [00:28<00:00,  1.84it/s, loss=0.0014]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/10], Train Loss: 0.0022, Val Loss: 0.0029, Time: 120.65s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/10 [Train]: 100%|██████████| 167/167 [01:32<00:00,  1.81it/s, loss=0.0014]\n",
            "Epoch 3/10 [Val]: 100%|██████████| 52/52 [00:28<00:00,  1.84it/s, loss=0.0016]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [3/10], Train Loss: 0.0015, Val Loss: 0.0028, Time: 120.47s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/10 [Train]: 100%|██████████| 167/167 [01:32<00:00,  1.81it/s, loss=0.0003]\n",
            "Epoch 4/10 [Val]: 100%|██████████| 52/52 [00:28<00:00,  1.84it/s, loss=0.0014]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [4/10], Train Loss: 0.0013, Val Loss: 0.0028, Time: 120.32s\n",
            "Early stopping triggered after 4 epochs. No improvement for 3 epochs.\n",
            "Average epoch training time: 120.46s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Testing split_13: 100%|██████████| 23/23 [00:12<00:00,  1.86it/s, loss=0.0007]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "split_13 Test Loss: 0.0014, Epochs Trained: 4\n",
            "\n",
            "Training on split_14\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/10 [Train]: 100%|██████████| 167/167 [01:32<00:00,  1.81it/s, loss=0.0014]\n",
            "Epoch 1/10 [Val]: 100%|██████████| 48/48 [00:26<00:00,  1.84it/s, loss=0.0020]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/10], Train Loss: 0.0039, Val Loss: 0.0034, Time: 118.45s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/10 [Train]: 100%|██████████| 167/167 [01:32<00:00,  1.81it/s, loss=0.0017]\n",
            "Epoch 2/10 [Val]: 100%|██████████| 48/48 [00:26<00:00,  1.84it/s, loss=0.0013]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/10], Train Loss: 0.0017, Val Loss: 0.0035, Time: 118.24s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/10 [Train]: 100%|██████████| 167/167 [01:32<00:00,  1.81it/s, loss=0.0003]\n",
            "Epoch 3/10 [Val]: 100%|██████████| 48/48 [00:26<00:00,  1.84it/s, loss=0.0012]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [3/10], Train Loss: 0.0013, Val Loss: 0.0035, Time: 118.27s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/10 [Train]: 100%|██████████| 167/167 [01:32<00:00,  1.80it/s, loss=0.0010]\n",
            "Epoch 4/10 [Val]: 100%|██████████| 48/48 [00:26<00:00,  1.83it/s, loss=0.0011]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [4/10], Train Loss: 0.0011, Val Loss: 0.0036, Time: 118.81s\n",
            "Early stopping triggered after 4 epochs. No improvement for 3 epochs.\n",
            "Average epoch training time: 118.44s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Testing split_14: 100%|██████████| 27/27 [00:14<00:00,  1.85it/s, loss=0.0025]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "split_14 Test Loss: 0.0013, Epochs Trained: 4\n",
            "\n",
            "Training on split_15\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/10 [Train]: 100%|██████████| 163/163 [01:30<00:00,  1.81it/s, loss=0.0001]\n",
            "Epoch 1/10 [Val]: 100%|██████████| 50/50 [00:27<00:00,  1.85it/s, loss=0.0003]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/10], Train Loss: 0.0038, Val Loss: 0.0014, Time: 117.12s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/10 [Train]: 100%|██████████| 163/163 [01:30<00:00,  1.81it/s, loss=0.0000]\n",
            "Epoch 2/10 [Val]: 100%|██████████| 50/50 [00:26<00:00,  1.86it/s, loss=0.0005]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/10], Train Loss: 0.0022, Val Loss: 0.0013, Time: 117.07s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/10 [Train]: 100%|██████████| 163/163 [01:29<00:00,  1.81it/s, loss=0.0015]\n",
            "Epoch 3/10 [Val]: 100%|██████████| 50/50 [00:26<00:00,  1.86it/s, loss=0.0004]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [3/10], Train Loss: 0.0019, Val Loss: 0.0012, Time: 116.82s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/10 [Train]: 100%|██████████| 163/163 [01:30<00:00,  1.81it/s, loss=0.0005]\n",
            "Epoch 4/10 [Val]: 100%|██████████| 50/50 [00:26<00:00,  1.86it/s, loss=0.0004]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [4/10], Train Loss: 0.0017, Val Loss: 0.0012, Time: 117.15s\n",
            "Early stopping triggered after 4 epochs. No improvement for 3 epochs.\n",
            "Average epoch training time: 117.04s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Testing split_15: 100%|██████████| 28/28 [00:14<00:00,  1.90it/s, loss=0.0008]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "split_15 Test Loss: 0.0008, Epochs Trained: 4\n",
            "\n",
            "Training on split_16\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/10 [Train]: 100%|██████████| 156/156 [01:26<00:00,  1.80it/s, loss=0.0024]\n",
            "Epoch 1/10 [Val]: 100%|██████████| 54/54 [00:29<00:00,  1.84it/s, loss=0.0009]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/10], Train Loss: 0.0041, Val Loss: 0.0011, Time: 115.83s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/10 [Train]: 100%|██████████| 156/156 [01:26<00:00,  1.81it/s, loss=0.0012]\n",
            "Epoch 2/10 [Val]: 100%|██████████| 54/54 [00:29<00:00,  1.84it/s, loss=0.0007]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/10], Train Loss: 0.0024, Val Loss: 0.0010, Time: 115.74s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/10 [Train]: 100%|██████████| 156/156 [01:26<00:00,  1.81it/s, loss=0.0040]\n",
            "Epoch 3/10 [Val]: 100%|██████████| 54/54 [00:29<00:00,  1.84it/s, loss=0.0007]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [3/10], Train Loss: 0.0020, Val Loss: 0.0010, Time: 115.43s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/10 [Train]: 100%|██████████| 156/156 [01:25<00:00,  1.82it/s, loss=0.0008]\n",
            "Epoch 4/10 [Val]: 100%|██████████| 54/54 [00:29<00:00,  1.85it/s, loss=0.0006]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [4/10], Train Loss: 0.0018, Val Loss: 0.0010, Time: 114.92s\n",
            "Early stopping triggered after 4 epochs. No improvement for 3 epochs.\n",
            "Average epoch training time: 115.48s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Testing split_16: 100%|██████████| 25/25 [00:13<00:00,  1.86it/s, loss=0.0001]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "split_16 Test Loss: 0.0007, Epochs Trained: 4\n",
            "\n",
            "Training completed for all splits.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Train on each split\n",
        "finbert_results = {}\n",
        "\n",
        "for split_name, loaders in finbert_data_loaders.items():\n",
        "    print(f\"\\nTraining on {split_name}\")\n",
        "    \n",
        "    # Initialize model, criterion, optimizer for this split\n",
        "    model = FinBERTReturnPredictor().to(device)\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    \n",
        "    # Early stopping parameters\n",
        "    min_epochs = 4  # Train for at least 4 epochs\n",
        "    max_epochs = 10  # Train for at most 10 epochs\n",
        "    patience = 3  # Stop if no improvement for 3 consecutive epochs\n",
        "    min_delta = 0.001  # Minimum improvement threshold\n",
        "    \n",
        "    best_val_loss = float('inf')\n",
        "    epochs_no_improve = 0\n",
        "    best_model_state = None\n",
        "    \n",
        "    train_loader = loaders['train']\n",
        "    val_loader = loaders['val']\n",
        "    test_loader = loaders['test']\n",
        "    \n",
        "    # Train the model\n",
        "    total_training_time = 0\n",
        "    for epoch in range(max_epochs):\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        epoch_start_time = time.time()\n",
        "        \n",
        "        # Add progress bar to display training progress\n",
        "        train_progress = tqdm(train_loader, desc=f'Epoch {epoch+1}/{max_epochs} [Train]')\n",
        "        for batch_finbert, batch_returns in train_progress:\n",
        "            batch_finbert = batch_finbert.to(device)\n",
        "            batch_returns = batch_returns.to(device)\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(batch_finbert)\n",
        "            loss = criterion(outputs, batch_returns)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            train_loss += loss.item()\n",
        "            # Update progress bar with current loss\n",
        "            train_progress.set_postfix(loss=f'{loss.item():.4f}')\n",
        "        \n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        with torch.no_grad():\n",
        "            # Add progress bar to display validation progress\n",
        "            val_progress = tqdm(val_loader, desc=f'Epoch {epoch+1}/{max_epochs} [Val]')\n",
        "            for batch_finbert, batch_returns in val_progress:\n",
        "                batch_finbert = batch_finbert.to(device)\n",
        "                batch_returns = batch_returns.to(device)\n",
        "                outputs = model(batch_finbert)            \n",
        "                current_loss = criterion(outputs, batch_returns).item()\n",
        "                val_loss += current_loss\n",
        "                # Update progress bar with current loss\n",
        "                val_progress.set_postfix(loss=f'{current_loss:.4f}')\n",
        "        \n",
        "        epoch_end_time = time.time()\n",
        "        epoch_duration = epoch_end_time - epoch_start_time\n",
        "        total_training_time += epoch_duration\n",
        "        \n",
        "        train_loss = train_loss / len(train_loader)\n",
        "        val_loss = val_loss / len(val_loader)\n",
        "        \n",
        "        train_losses.append(train_loss)\n",
        "        val_losses.append(val_loss)\n",
        "        \n",
        "        print(f'Epoch [{epoch+1}/{max_epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Time: {epoch_duration:.2f}s')\n",
        "        \n",
        "        # Early stopping logic\n",
        "        if val_loss < best_val_loss - min_delta:\n",
        "            # Significant improvement\n",
        "            best_val_loss = val_loss\n",
        "            epochs_no_improve = 0\n",
        "            # Save best model state\n",
        "            best_model_state = model.state_dict().copy()\n",
        "        else:\n",
        "            # No significant improvement\n",
        "            epochs_no_improve += 1\n",
        "            \n",
        "        # Check if training should stop\n",
        "        if epoch + 1 >= min_epochs and epochs_no_improve >= patience:\n",
        "            print(f\"Early stopping triggered after {epoch+1} epochs. No improvement for {epochs_no_improve} epochs.\")\n",
        "            # Restore to best model state\n",
        "            if best_model_state is not None:\n",
        "                model.load_state_dict(best_model_state)\n",
        "            break\n",
        "    \n",
        "    avg_epoch_time = total_training_time / (epoch + 1)\n",
        "    print(f'Average epoch training time: {avg_epoch_time:.2f}s')\n",
        "    \n",
        "    # Test evaluation\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    predictions = []\n",
        "    actuals = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        # Add progress bar to display testing progress\n",
        "        test_progress = tqdm(test_loader, desc=f'Testing {split_name}')\n",
        "        for batch_finbert, batch_returns in test_progress:\n",
        "            batch_finbert = batch_finbert.to(device)\n",
        "            batch_returns = batch_returns.to(device)\n",
        "            outputs = model(batch_finbert)\n",
        "            current_loss = criterion(outputs, batch_returns).item()\n",
        "            test_loss += current_loss\n",
        "            \n",
        "            predictions.extend(outputs.cpu().numpy().flatten())\n",
        "            actuals.extend(batch_returns.cpu().numpy().flatten())\n",
        "            # Update progress bar with current loss\n",
        "            test_progress.set_postfix(loss=f'{current_loss:.4f}')\n",
        "    \n",
        "    test_loss = test_loss / len(test_loader)\n",
        "    \n",
        "    # Store results for this split\n",
        "    finbert_results[split_name] = {\n",
        "        'model': model,\n",
        "        'train_losses': train_losses,\n",
        "        'val_losses': val_losses,\n",
        "        'test_loss': test_loss,\n",
        "        'predictions': predictions,\n",
        "        'actuals': actuals,\n",
        "        'avg_epoch_time': avg_epoch_time,\n",
        "        'epochs_trained': epoch + 1\n",
        "    }\n",
        "    \n",
        "    print(f\"{split_name} Test Loss: {test_loss:.4f}, Epochs Trained: {epoch+1}\")\n",
        "\n",
        "print(\"\\nTraining completed for all splits.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_r2(y_true, y_pred, in_sample=True, benchmark=None):\n",
        "    if in_sample:\n",
        "        return 1 - (np.sum((y_true - y_pred) ** 2) / \n",
        "                    np.sum((y_true - np.mean(y_true)) ** 2))\n",
        "    else:\n",
        "        if benchmark is None:\n",
        "            raise ValueError(\"Benchmark must be provided for out-of-sample R-squared calculation.\")\n",
        "        return 1 - (np.sum((y_true - y_pred) ** 2) / \n",
        "                    np.sum((y_true - benchmark) ** 2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "split_1 Test Results:\n",
            "R² Score: -0.212674\n",
            "MSE: 0.000435\n",
            "\n",
            "split_2 Test Results:\n",
            "R² Score: -0.015538\n",
            "MSE: 0.000699\n",
            "\n",
            "split_3 Test Results:\n",
            "R² Score: -0.174191\n",
            "MSE: 0.001021\n",
            "\n",
            "split_4 Test Results:\n",
            "R² Score: -0.287692\n",
            "MSE: 0.001618\n",
            "\n",
            "split_5 Test Results:\n",
            "R² Score: -0.074915\n",
            "MSE: 0.001851\n",
            "\n",
            "split_6 Test Results:\n",
            "R² Score: -0.096674\n",
            "MSE: 0.000597\n",
            "\n",
            "split_7 Test Results:\n",
            "R² Score: -0.295921\n",
            "MSE: 0.000612\n",
            "\n",
            "split_8 Test Results:\n",
            "R² Score: -0.208950\n",
            "MSE: 0.000703\n",
            "\n",
            "split_9 Test Results:\n",
            "R² Score: -0.080251\n",
            "MSE: 0.001350\n",
            "\n",
            "split_10 Test Results:\n",
            "R² Score: -0.086370\n",
            "MSE: 0.000675\n",
            "\n",
            "split_11 Test Results:\n",
            "R² Score: -0.124485\n",
            "MSE: 0.000759\n",
            "\n",
            "split_12 Test Results:\n",
            "R² Score: -0.053566\n",
            "MSE: 0.005150\n",
            "\n",
            "split_13 Test Results:\n",
            "R² Score: -0.060558\n",
            "MSE: 0.001401\n",
            "\n",
            "split_14 Test Results:\n",
            "R² Score: -0.127904\n",
            "MSE: 0.001305\n",
            "\n",
            "split_15 Test Results:\n",
            "R² Score: -0.079212\n",
            "MSE: 0.000774\n",
            "\n",
            "split_16 Test Results:\n",
            "R² Score: -0.008780\n",
            "MSE: 0.000732\n",
            "\n",
            "Evaluation Metrics Summary for All Splits:\n",
            "       Split        R²       MSE\n",
            "0    split_1 -0.212674  0.000435\n",
            "1    split_2 -0.015538  0.000699\n",
            "2    split_3 -0.174191  0.001021\n",
            "3    split_4 -0.287692  0.001618\n",
            "4    split_5 -0.074915  0.001851\n",
            "5    split_6 -0.096674  0.000597\n",
            "6    split_7 -0.295921  0.000612\n",
            "7    split_8 -0.208950  0.000703\n",
            "8    split_9 -0.080251  0.001350\n",
            "9   split_10 -0.086370  0.000675\n",
            "10  split_11 -0.124485  0.000759\n",
            "11  split_12 -0.053566  0.005150\n",
            "12  split_13 -0.060558  0.001401\n",
            "13  split_14 -0.127904  0.001305\n",
            "14  split_15 -0.079212  0.000774\n",
            "15  split_16 -0.008780  0.000732\n"
          ]
        }
      ],
      "source": [
        "# Evaluate model performance across all splits\n",
        "finbert_evaluation_results = {}\n",
        "\n",
        "for split_name, split_data in finbert_results.items():\n",
        "    model = split_data['model']\n",
        "    test_loader = finbert_data_loaders[split_name]['test']\n",
        "    \n",
        "    # Evaluate model performance on the test set\n",
        "    model.eval()\n",
        "    test_predictions = []\n",
        "    test_actuals = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch_finbert, batch_returns in test_loader:\n",
        "            batch_finbert = batch_finbert.to(device)\n",
        "            outputs = model(batch_finbert)\n",
        "            test_predictions.extend(outputs.cpu().numpy())\n",
        "            test_actuals.extend(batch_returns.numpy())\n",
        "    \n",
        "    test_predictions = np.array(test_predictions)\n",
        "    test_actuals = np.array(test_actuals)\n",
        "    \n",
        "    # Calculate R² and MSE\n",
        "    r2 = calculate_r2(test_actuals, test_predictions, in_sample=False, benchmark=0)\n",
        "    mse = np.mean((test_predictions - test_actuals) ** 2)\n",
        "    \n",
        "    # Store evaluation results\n",
        "    finbert_evaluation_results[split_name] = {\n",
        "        'r2': r2,\n",
        "        'mse': mse\n",
        "    }\n",
        "    \n",
        "    # Print evaluation metrics\n",
        "    print(f\"\\n{split_name} Test Results:\")\n",
        "    print(f\"R² Score: {r2:.6f}\")\n",
        "    print(f\"MSE: {mse:.6f}\")\n",
        "\n",
        "# Create summary table of evaluation results\n",
        "import pandas as pd\n",
        "\n",
        "# Extract metrics for each split\n",
        "summary_data = []\n",
        "for split_name, metrics in finbert_evaluation_results.items():\n",
        "    summary_data.append({\n",
        "        'Split': split_name,\n",
        "        'R²': metrics['r2'],\n",
        "        'MSE': metrics['mse']\n",
        "    })\n",
        "\n",
        "# Create summary DataFrame\n",
        "summary_df = pd.DataFrame(summary_data)\n",
        "print(\"\\nEvaluation Metrics Summary for All Splits:\")\n",
        "print(summary_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "All predictions saved to news_w_sentiment_FineTuneFinBERT.csv, total records: 13696\n",
            "\n",
            "Data preview:\n",
            "   publication_datetime                                              title  \\\n",
            "0            2018-05-01                                Still Hot For Cocoa   \n",
            "22           2018-05-01                      T-Mobile Must Stay a Maverick   \n",
            "21           2018-05-01  Business News: Deal to Forge Biggest U.S. Refiner   \n",
            "20           2018-05-01  U.S. News: High Court to Heighten Class-Action...   \n",
            "19           2018-05-01             Walmart Pulls Back From U.K. Groceries   \n",
            "\n",
            "                                                 body tickers  sentiment  \n",
            "0   That compares with aroughly 7% gain for a Gold...     HSY  -0.002577  \n",
            "22  The politically important sideshow will be whe...    TMUS   0.002361  \n",
            "21  Marathon's shares have nearly tripled since th...     MPC   0.007085  \n",
            "20  But rather than try to distribute that money a...   GOOGL  -0.005097  \n",
            "19  Walmart bought the U.K.'s third-largest grocer...     WMT   0.008098  \n"
          ]
        }
      ],
      "source": [
        "# Save test set predictions for each split\n",
        "all_test_dfs = []\n",
        "\n",
        "for split_name, split_data in finbert_results.items():\n",
        "    # Get corresponding test set data and predictions\n",
        "    test_set = rolling_splits[int(split_name.split('_')[1])-1]['test_set']\n",
        "    test_predictions = split_data['predictions']\n",
        "    \n",
        "    # Copy test data and add predictions as sentiment\n",
        "    test_df = test_set.copy()\n",
        "    test_df['sentiment'] = test_predictions\n",
        "    \n",
        "    # Keep only necessary columns\n",
        "    keep_columns = ['publication_datetime', 'title', 'body', 'tickers', 'sentiment']\n",
        "    test_df = test_df[keep_columns]\n",
        "    \n",
        "    # Add to the list\n",
        "    all_test_dfs.append(test_df)\n",
        "\n",
        "# Merge all test data\n",
        "if all_test_dfs:\n",
        "    all_test_predictions = pd.concat(all_test_dfs, ignore_index=True)\n",
        "    \n",
        "    # Sort by date\n",
        "    all_test_predictions['publication_datetime'] = pd.to_datetime(all_test_predictions['publication_datetime'])\n",
        "    all_test_predictions = all_test_predictions.sort_values('publication_datetime')\n",
        "    \n",
        "    # Save the merged predictions\n",
        "    all_test_predictions.to_csv(\"news_w_sentiment_FineTuneFinBERT.csv\", index=False)\n",
        "    print(f\"All predictions saved to news_w_sentiment_FineTuneFinBERT.csv, total records: {len(all_test_predictions)}\")\n",
        "    \n",
        "    # Display the first few rows to preview the results\n",
        "    print(\"\\nData preview:\")\n",
        "    print(all_test_predictions.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "All model results saved to 'fine-tuned_finbert_model_results' directory\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create directory to save results\n",
        "os.makedirs(\"fine-tuned_finbert_model_results\", exist_ok=True)\n",
        "\n",
        "# Save results for each split\n",
        "for split_name, split_result in finbert_results.items():\n",
        "    # Create subdirectory for each split\n",
        "    split_dir = os.path.join(\"fine-tuned_finbert_model_results\", split_name)\n",
        "    os.makedirs(split_dir, exist_ok=True)\n",
        "    \n",
        "    # 1. Save model weights\n",
        "    model_path = os.path.join(split_dir, f\"{split_name}_model.pth\")\n",
        "    torch.save(split_result['model'].state_dict(), model_path)\n",
        "    \n",
        "    # 2. Save training and validation loss history\n",
        "    losses = {\n",
        "        'train_losses': split_result['train_losses'],\n",
        "        'val_losses': split_result['val_losses'],\n",
        "        'test_loss': split_result['test_loss']\n",
        "    }\n",
        "    \n",
        "    # Convert NumPy arrays to Python lists for JSON serialization\n",
        "    for key in losses:\n",
        "        if isinstance(losses[key], (np.ndarray, list)):\n",
        "            losses[key] = [float(x) for x in losses[key]]\n",
        "        elif isinstance(losses[key], (np.float32, np.float64)):\n",
        "            losses[key] = float(losses[key])\n",
        "    \n",
        "    # Save loss data\n",
        "    with open(os.path.join(split_dir, f\"{split_name}_losses.json\"), 'w') as f:\n",
        "        json.dump(losses, f, indent=4)\n",
        "    \n",
        "    # 3. Save prediction results\n",
        "    predictions_data = {\n",
        "        'predictions': [float(p) for p in split_result['predictions']],\n",
        "        'actuals': [float(a) for a in split_result['actuals']]\n",
        "    }\n",
        "    with open(os.path.join(split_dir, f\"{split_name}_predictions.json\"), 'w') as f:\n",
        "        json.dump(predictions_data, f, indent=4)\n",
        "    \n",
        "    # 4. Plot and save loss curves\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(split_result['train_losses'], label='Train Loss')\n",
        "    plt.plot(split_result['val_losses'], label='Validation Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title(f'{split_name} - Training and Validation Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.savefig(os.path.join(split_dir, f\"{split_name}_loss_curve.png\"))\n",
        "    plt.close()\n",
        "    \n",
        "    # 5. Plot prediction vs actual values scatter plot\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.scatter(split_result['actuals'], split_result['predictions'], alpha=0.5)\n",
        "    plt.plot([-0.1, 0.1], [-0.1, 0.1], 'r--')  # Ideal line\n",
        "    plt.xlabel('Actual Returns')\n",
        "    plt.ylabel('Predicted Returns')\n",
        "    plt.title(f'{split_name} - Prediction vs Actual')\n",
        "    plt.grid(True)\n",
        "    plt.savefig(os.path.join(split_dir, f\"{split_name}_prediction_scatter.png\"))\n",
        "    plt.close()\n",
        "\n",
        "# Create README file with explanation of results\n",
        "readme_content = \"\"\"# Financial News Sentiment Analysis with FinBERT\n",
        "\n",
        "## Model Structure\n",
        "- FinBERT model for extracting news text features\n",
        "- Fully connected layers for stock return prediction\n",
        "- Trained using MSE loss function and Adam optimizer\n",
        "\n",
        "## File Descriptions\n",
        "Each time window split contains the following files:\n",
        "- `{split_name}_model.pth`: Trained model weights\n",
        "- `{split_name}_losses.json`: Training and validation loss values\n",
        "- `{split_name}_predictions.json`: Predictions and actual values on test set\n",
        "- `{split_name}_loss_curve.png`: Loss curve chart\n",
        "- `{split_name}_prediction_scatter.png`: Prediction vs actual values scatter plot\n",
        "\n",
        "## How to Load and Use the Model\n",
        "\"\"\"\n",
        "\n",
        "with open(os.path.join(\"fine-tuned_finbert_model_results\", \"README.md\"), 'w') as f:\n",
        "    f.write(readme_content)\n",
        "\n",
        "print(\"All model results saved to 'fine-tuned_finbert_model_results' directory\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
